{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to K3IM: Keras 3 Image Models","text":""},{"location":"1d_models/","title":"1D Models","text":"<p><code>k3im.cait_1d.CAiT_1DModel</code> </p> <p>An extention of Class Attention in Image Transformers (CAiT) reimplemented for 1D Data.  The model expects 1D data of shape <code>(batch, seq_len, channels)</code></p> <p>Parameters:</p> Name Type Description Default <code>`seq_len`</code> <p>number of steps</p> required <code>`patch_size`</code> <p>number steps in a patch </p> required <code>`num_classes`</code> <p>output classes for classification</p> required <code>`dim`</code> <p>projection dim for patches,</p> required <code>`dim_head`</code> <p>size of each attention head</p> required <code>`mlp_dim`</code> <p>Projection Dim in transformer after each MultiHeadAttention layer</p> required <code>`depth`</code> <p>number of patch transformer units</p> required <code>`cls_depth`</code> <p>number of transformer units applied to class attention transformer</p> required <code>`heads`</code> <p>number of attention heads</p> required <code>`channels`</code> <p>number of features/channels in the input default <code>3</code></p> required <code>`dropout_rate`</code> <p>dropout applied to MultiHeadAttention in class and patch transformers</p> required Source code in <code>k3im/cait_1d.py</code> <pre><code>def CAiT_1DModel(\n    seq_len,\n    patch_size,\n    num_classes,\n    dim,\n    dim_head,\n    mlp_dim,\n    depth,\n    cls_depth,\n    heads,\n    channels=3,\n    dropout_rate=0.0,\n):\n    \"\"\" \n    An extention of Class Attention in Image Transformers (CAiT) reimplemented for 1D Data. \n    The model expects 1D data of shape `(batch, seq_len, channels)`\n\n    Args:\n        `seq_len`: number of steps\n        `patch_size`: number steps in a patch \n        `num_classes`: output classes for classification\n        `dim`: projection dim for patches,\n        `dim_head`: size of each attention head\n        `mlp_dim`: Projection Dim in transformer after each MultiHeadAttention layer\n        `depth`: number of patch transformer units\n        `cls_depth`: number of transformer units applied to class attention transformer\n        `heads`: number of attention heads\n        `channels`: number of features/channels in the input default `3`\n        `dropout_rate`: dropout applied to MultiHeadAttention in class and patch transformers\n    \"\"\"\n    assert seq_len % patch_size == 0\n    patch_dim = channels * patch_size\n    i_p = layers.Input((seq_len, channels))\n    patches = layers.Reshape((-1, patch_dim))(i_p)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    pos_embedding = posemb_sincos_1d(patches)\n    patches += pos_embedding\n    dim = ops.shape(patches)[-1]\n    patches = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout_rate=dropout_rate)(patches)\n    _, cls_token = CLS_Token(dim)(patches)\n    cls_token = Transformer(dim, cls_depth, heads, dim_head, mlp_dim, dropout_rate=dropout_rate)(\n        cls_token, patches\n    )\n    cls_token = ops.squeeze(cls_token, axis=1)\n    o_p = layers.Dense(num_classes)(cls_token)\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> <p><code>k3im.cct_1d.CCT_1DModel</code> </p> <p>Create a Convolutional Transformer for sequences.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <p>A tuple of (seq_len, num_channels).</p> required <code>num_heads</code> <p>An integer.</p> required <code>projection_dim</code> <p>An integer representing the projection dimension.</p> required <code>kernel_size</code> <p>An integer representing the size of the convolution window.</p> required <code>stride</code> <p>An integer representing the stride of the convolution.</p> required <code>padding</code> <p>One of 'valid', 'same' or 'causal'. Causal is for decoding.</p> required <code>transformer_units</code> <p>A list of integers representing the number of units in each transformer layer.</p> required <code>stochastic_depth_rate</code> <p>A float representing the drop probability for the stochastic depth layer.</p> required <code>transformer_layers</code> <p>An integer representing the number of transformer layers.</p> required <code>num_classes</code> <p>An integer representing the number of classes for classification.</p> required <code>positional_emb</code> <p>Boolean, whether to use positional embeddings.</p> <code>False</code> Source code in <code>k3im/cct_1d.py</code> <pre><code>def CCT_1DModel(\n    input_shape,\n    num_heads,\n    projection_dim,\n    kernel_size,\n    stride,\n    padding,\n    transformer_units,\n    stochastic_depth_rate,\n    transformer_layers,\n    num_classes,\n    positional_emb=False,\n):\n    \"\"\"\n    Create a Convolutional Transformer for sequences.\n\n    Args:\n        input_shape: A tuple of (seq_len, num_channels).\n        num_heads: An integer.\n        projection_dim: An integer representing the projection dimension.\n        kernel_size: An integer representing the size of the convolution window.\n        stride: An integer representing the stride of the convolution.\n        padding: One of 'valid', 'same' or 'causal'. Causal is for decoding.\n        transformer_units: A list of integers representing the number of units\n            in each transformer layer.\n        stochastic_depth_rate: A float representing the drop probability for the\n            stochastic depth layer.\n        transformer_layers: An integer representing the number of transformer layers.\n        num_classes: An integer representing the number of classes for classification.\n        positional_emb: Boolean, whether to use positional embeddings.\n\n    \"\"\"\n    inputs = layers.Input(input_shape)\n\n    # Encode patches.\n\n    cct_tokenizer = CCTTokenizer1D(\n        kernel_size,\n        stride,\n        padding,\n        n_output_channels=[64, projection_dim],\n        n_conv_layers=2,\n    )\n    encoded_patches = cct_tokenizer(inputs)\n\n    # Apply positional embedding.\n    if positional_emb:\n        sequence_length = encoded_patches.shape[1]\n        encoded_patches += PositionEmbedding(sequence_length=sequence_length)(\n            encoded_patches\n        )\n\n    # Calculate Stochastic Depth probabilities.\n    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n\n    # Create multiple layers of the Transformer block.\n    for i in range(transformer_layers):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n\n        # Create a multi-head attention layer.\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n        )(x1, x1)\n\n        # Skip connection 1.\n        attention_output = StochasticDepth(dpr[i])(attention_output)\n        x2 = layers.Add()([attention_output, encoded_patches])\n\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n\n        # MLP.\n        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n\n        # Skip connection 2.\n        x3 = StochasticDepth(dpr[i])(x3)\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Apply sequence pooling.\n    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n    weighted_representation = SequencePooling()(representation)\n\n    # Classify outputs.\n    logits = layers.Dense(num_classes)(weighted_representation)\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=logits)\n    return model\n</code></pre> <p><code>k3im.convmixer_1d.ConvMixer1DModel</code> </p> <p>ConvMixer model for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>`seq_len`</code> <p>number of steps</p> required <code>`n_features`</code> <p>number of features/channels in the input default <code>3</code></p> required <code>`filters`</code> <p>number of filters in the convolutional stem</p> required <code>`depth`</code> <p>number of conv mixer blocks</p> required <code>`kernel_size`</code> <p>kernel size for the depthwise convolution</p> required <code>`patch_size`</code> <p>number steps in a patch</p> required <code>`num_classes`</code> <p>output classes for classification</p> required Source code in <code>k3im/convmixer_1d.py</code> <pre><code>def ConvMixer1DModel(\n    seq_len=32,\n    n_features=3,\n    filters=256,\n    depth=8,\n    kernel_size=5,\n    patch_size=2,\n    num_classes=10,\n):\n    \"\"\" \n    ConvMixer model for 1D data.\n\n    Args:\n        `seq_len`: number of steps\n        `n_features`: number of features/channels in the input default `3`\n        `filters`: number of filters in the convolutional stem\n        `depth`: number of conv mixer blocks\n        `kernel_size`: kernel size for the depthwise convolution\n        `patch_size`: number steps in a patch\n        `num_classes`: output classes for classification\n\n    \"\"\"\n    inputs = keras.Input((seq_len, n_features))\n\n    # Extract patch embeddings.\n    x = conv_stem(inputs, filters, patch_size)\n\n    # ConvMixer blocks.\n    for _ in range(depth):\n        x = conv_mixer_block(x, filters, kernel_size)\n\n    # Classification block.\n    x = layers.GlobalAvgPool1D()(x)\n    outputs = layers.Dense(num_classes)(x)\n\n    return keras.Model(inputs, outputs)\n</code></pre> <p><code>k3im.eanet_1d.EANet1DModel</code> </p> <p>Create an External Attention Network for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>`seq_len`</code> <p>number of steps</p> required <code>`patch_size`</code> <p>number steps in a patch</p> required <code>`num_classes`</code> <p>output classes for classification</p> required <code>`dim`</code> <p>projection dim for patches,</p> required <code>`depth`</code> <p>number of patch transformer units</p> required <code>`heads`</code> <p>number of attention heads</p> required <code>`mlp_dim`</code> <p>Projection Dim in transformer after each MultiHeadAttention layer</p> required <code>`dim_coefficient`</code> <p>coefficient for increasing the number of heads</p> required <code>`attention_dropout`</code> <p>dropout applied to MultiHeadAttention in class and patch transformers</p> required <code>`channels`</code> <p>number of features/channels in the input default <code>3</code></p> required Source code in <code>k3im/eanet_1d.py</code> <pre><code>def EANet1DModel(\n    seq_len,\n    patch_size,\n    num_classes,\n    dim,\n    depth,\n    heads,\n    mlp_dim,\n    dim_coefficient=4,\n    attention_dropout=0.0,\n    channels=3,\n):\n    \"\"\"\n    Create an External Attention Network for 1D data.\n\n    Args:\n        `seq_len`: number of steps\n        `patch_size`: number steps in a patch\n        `num_classes`: output classes for classification\n        `dim`: projection dim for patches,\n        `depth`: number of patch transformer units\n        `heads`: number of attention heads\n        `mlp_dim`: Projection Dim in transformer after each MultiHeadAttention layer\n        `dim_coefficient`: coefficient for increasing the number of heads\n        `attention_dropout`: dropout applied to MultiHeadAttention in class and patch transformers\n        `channels`: number of features/channels in the input default `3`\n\n    \"\"\"\n    assert seq_len % patch_size == 0\n    patch_dim = channels * patch_size\n    i_p = layers.Input((seq_len, channels))\n    patches = layers.Reshape((-1, patch_dim))(i_p)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    pos_embedding = posemb_sincos_1d(patches)\n    patches += pos_embedding\n    patches = Transformer(\n        dim=dim,\n        depth=depth,\n        heads=heads,\n        mlp_dim=mlp_dim,\n        dim_coefficient=dim_coefficient,\n        attention_dropout=attention_dropout,\n        projection_dropout=0,\n    )(patches)\n    patches = layers.GlobalAveragePooling1D(name=\"avg_pool\")(patches)\n    o_p = layers.Dense(num_classes)(patches)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> <p><code>k3im.fnet_1d.FNet1DModel</code> </p> <p>Instantiate a FNet model for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>seq_len</code> <p>An integer representing the number of steps in the input sequence.</p> required <code>patch_size</code> <p>An integer representing the number of steps in a patch (default=4).  </p> required <code>num_classes</code> <p>An integer representing the number of classes for classification.</p> required <code>dim</code> <p>An integer representing the projection dimension.</p> required <code>depth</code> <p>An integer representing the number of transformer layers.</p> required <code>channels</code> <p>An integer representing the number of channels in the input.</p> <code>3</code> <code>dropout_rate</code> <p>A float representing the dropout rate.</p> <code>0.0</code> Source code in <code>k3im/fnet_1d.py</code> <pre><code>def FNet1DModel(\n    seq_len, patch_size, num_classes, dim, depth, channels=3, dropout_rate=0.0\n):\n    \"\"\"\n    Instantiate a FNet model for 1D data.\n\n    Args:\n        seq_len: An integer representing the number of steps in the input sequence.\n        patch_size: An integer representing the number of steps in a\n            patch (default=4).  \n        num_classes: An integer representing the number of classes for classification.\n        dim: An integer representing the projection dimension.\n        depth: An integer representing the number of transformer layers.\n        channels: An integer representing the number of channels in the input.\n        dropout_rate: A float representing the dropout rate.\n    \"\"\"\n    assert seq_len % patch_size == 0\n    num_patches = seq_len // patch_size\n    patch_dim = channels * patch_size\n    i_p = layers.Input((seq_len, channels))\n    patches = layers.Reshape((-1, patch_dim))(i_p)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    pos_embedding = posemb_sincos_1d(patches)\n    patches += pos_embedding\n    dim = ops.shape(patches)[-1]\n    for _ in range(depth):\n        patches = FNetLayer(dim, dropout_rate)(patches)\n    patches = layers.GlobalAveragePooling1D(name=\"avg_pool\")(patches)\n    o_p = layers.Dense(num_classes)(patches)\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> <p><code>k3im.gmlp_1d.gMLP1DModel</code> </p> <p>Instantiate a gMLP model for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>seq_len</code> <p>An integer representing the number of steps in the input sequence.</p> required <code>patch_size</code> <p>An integer representing the number of steps in a patch (default=4).</p> required <code>num_classes</code> <p>An integer representing the number of classes for classification.</p> required <code>dim</code> <p>An integer representing the projection dimension.</p> required <code>depth</code> <p>An integer representing the number of transformer layers.</p> required <code>channels</code> <p>An integer representing the number of channels in the input.</p> <code>3</code> <code>dropout_rate</code> <p>A float representing the dropout rate.</p> <code>0.0</code> Source code in <code>k3im/gmlp_1d.py</code> <pre><code>def gMLP1DModel(\n    seq_len, patch_size, num_classes, dim, depth, channels=3, dropout_rate=0.0\n):\n    \"\"\"Instantiate a gMLP model for 1D data.\n\n    Args:\n        seq_len: An integer representing the number of steps in the input sequence.\n        patch_size: An integer representing the number of steps in a\n            patch (default=4).\n        num_classes: An integer representing the number of classes for classification.\n        dim: An integer representing the projection dimension.\n        depth: An integer representing the number of transformer layers.\n        channels: An integer representing the number of channels in the input.\n        dropout_rate: A float representing the dropout rate.\n\n        \"\"\"\n    assert seq_len % patch_size == 0\n    num_patches = seq_len // patch_size\n    patch_dim = channels * patch_size\n    i_p = layers.Input((seq_len, channels))\n    patches = layers.Reshape((-1, patch_dim))(i_p)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    pos_embedding = posemb_sincos_1d(patches)\n    patches += pos_embedding\n    dim = ops.shape(patches)[-1]\n    for _ in range(depth):\n        patches = gMLPLayer(num_patches, dim, dropout_rate)(patches)\n    patches = layers.GlobalAveragePooling1D(name=\"avg_pool\")(patches)\n    o_p = layers.Dense(num_classes)(patches)\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> <p><code>k3im.mlp_mixer_1d.Mixer1DModel</code> </p> <p>Instantiate a Mixer model for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>seq_len</code> <p>An integer representing the number of steps in the input sequence.</p> required <code>patch_size</code> <p>An integer representing the number of steps in a patch (default=4).</p> required <code>num_classes</code> <p>An integer representing the number of classes for classification.</p> required <code>dim</code> <p>An integer representing the projection dimension.</p> required <code>depth</code> <p>An integer representing the number of transformer layers.</p> required <code>channels</code> <p>An integer representing the number of channels in the input.</p> <code>3</code> <code>hidden_units</code> <p>An integer representing the number of hidden units in the MLP.</p> <code>64</code> <code>dropout_rate</code> <p>A float representing the dropout rate.</p> <code>0.0</code> Source code in <code>k3im/mlp_mixer_1d.py</code> <pre><code>def Mixer1DModel(\n    seq_len,\n    patch_size,\n    num_classes,\n    dim,\n    depth,\n    channels=3,\n    hidden_units=64,\n    dropout_rate=0.0,\n):\n    \"\"\"Instantiate a Mixer model for 1D data.\n\n    Args:\n        seq_len: An integer representing the number of steps in the input sequence.\n        patch_size: An integer representing the number of steps in a\n            patch (default=4).\n        num_classes: An integer representing the number of classes for classification.\n        dim: An integer representing the projection dimension.\n        depth: An integer representing the number of transformer layers.\n        channels: An integer representing the number of channels in the input.\n        hidden_units: An integer representing the number of hidden units in the MLP.\n        dropout_rate: A float representing the dropout rate.\n    \"\"\"\n    assert seq_len % patch_size == 0\n    num_patches = seq_len // patch_size\n    patch_dim = channels * patch_size\n    i_p = layers.Input((seq_len, channels))\n    patches = layers.Reshape((-1, patch_dim))(i_p)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    pos_embedding = posemb_sincos_1d(patches)\n    patches += pos_embedding\n    for _ in range(depth):\n        patches = MLPMixerLayer(num_patches, hidden_units, dropout_rate)(patches)\n    patches = layers.GlobalAveragePooling1D(name=\"avg_pool\")(patches)\n    o_p = layers.Dense(num_classes)(patches)\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> <p><code>k3im.simple_vit_1d.SimpleViT1DModel</code> </p> <p>Create a Simple Vision Transformer for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>`seq_len`</code> <p>number of steps</p> required <code>`patch_size`</code> <p>number steps in a patch</p> required <code>`num_classes`</code> <p>output classes for classification</p> required <code>`dim`</code> <p>projection dim for patches,</p> required <code>`depth`</code> <p>number of patch transformer units</p> required <code>`heads`</code> <p>number of attention heads</p> required <code>`mlp_dim`</code> <p>Projection Dim in transformer after each MultiHeadAttention layer</p> required <code>`channels`</code> <p>number of features/channels in the input default <code>3</code></p> required <code>`dim_head`</code> <p>size of each attention head</p> required Source code in <code>k3im/simple_vit_1d.py</code> <pre><code>def SimpleViT1DModel(\n    seq_len,\n    patch_size,\n    num_classes,\n    dim,\n    depth,\n    heads,\n    mlp_dim,\n    channels=3,\n    dim_head=64,\n):\n    \"\"\" Create a Simple Vision Transformer for 1D data.\n\n    Args:\n        `seq_len`: number of steps\n        `patch_size`: number steps in a patch\n        `num_classes`: output classes for classification\n        `dim`: projection dim for patches,\n        `depth`: number of patch transformer units\n        `heads`: number of attention heads\n        `mlp_dim`: Projection Dim in transformer after each MultiHeadAttention layer\n        `channels`: number of features/channels in the input default `3`\n        `dim_head`: size of each attention head\n    \"\"\"\n    assert seq_len % patch_size == 0\n    patch_dim = channels * patch_size\n    i_p = layers.Input((seq_len, channels))\n    patches = layers.Reshape((-1, patch_dim))(i_p)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    pos_embedding = posemb_sincos_1d(patches)\n    patches += pos_embedding\n    patches = Transformer(dim, depth, heads, dim_head, mlp_dim)(patches)\n    patches = layers.GlobalAveragePooling1D(name=\"avg_pool\")(patches)\n    o_p = layers.Dense(num_classes)(patches)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> <p><code>k3im.vit_1d.ViT1DModel</code> </p> <p>Create a Vision Transformer for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>`seq_len`</code> <p>number of steps</p> required <code>`patch_size`</code> <p>number steps in a patch</p> required <code>`num_classes`</code> <p>output classes for classification</p> required <code>`dim`</code> <p>projection dim for patches,</p> required <code>`depth`</code> <p>number of patch transformer units</p> required <code>`heads`</code> <p>number of attention heads</p> required <code>`mlp_dim`</code> <p>Projection Dim in transformer after each MultiHeadAttention layer</p> required <code>`channels`</code> <p>number of features/channels in the input default <code>3</code></p> required <code>`dim_head`</code> <p>size of each attention head</p> required Source code in <code>k3im/vit_1d.py</code> <pre><code>def ViT1DModel(\n    seq_len,\n    patch_size,\n    num_classes,\n    dim,\n    depth,\n    heads,\n    mlp_dim,\n    channels=3,\n    dim_head=64,\n):\n    \"\"\" Create a Vision Transformer for 1D data.\n\n    Args:\n        `seq_len`: number of steps\n        `patch_size`: number steps in a patch\n        `num_classes`: output classes for classification\n        `dim`: projection dim for patches,\n        `depth`: number of patch transformer units\n        `heads`: number of attention heads\n        `mlp_dim`: Projection Dim in transformer after each MultiHeadAttention layer\n        `channels`: number of features/channels in the input default `3`\n        `dim_head`: size of each attention head\n    \"\"\"\n    assert seq_len % patch_size == 0\n    patch_dim = channels * patch_size\n    i_p = layers.Input((seq_len, channels))\n    patches = layers.Reshape((-1, patch_dim))(i_p)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    num_patches = ops.shape(patches)[1]\n    patches = ClassTokenPositionEmb(num_patches, dim)(patches)\n    patches = Transformer(dim, depth, heads, dim_head, mlp_dim)(patches)\n    cls_tokens = patches[:, -1]\n    o_p = layers.Dense(num_classes)(cls_tokens)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre>"}]}