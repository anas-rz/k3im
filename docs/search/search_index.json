{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to K3IM: Keras 3 Image Models","text":"<p>K3IM empowers you with a rich collection of classification models tailored for images, 1D data, 3D structures, and spatiotemporal data. Built upon Keras 3, these models effortlessly work across TensorFlow, PyTorch, or JAX, offering you flexibility across different machine learning frameworks.</p> <p>You can install latest version using:</p> <p><code>pip install k3im</code></p>"},{"location":"1d_models/","title":"1D Models","text":"<p><code>k3im.cait_1d.CAiT_1DModel</code> </p> <p>An extention of Class Attention in Image Transformers (CAiT) reimplemented for 1D Data.  The model expects 1D data of shape <code>(batch, seq_len, channels)</code></p> <p>Parameters:</p> Name Type Description Default <code>`seq_len`</code> <p>number of steps</p> required <code>`patch_size`</code> <p>number steps in a patch </p> required <code>`num_classes`</code> <p>output classes for classification</p> required <code>`dim`</code> <p>projection dim for patches,</p> required <code>`dim_head`</code> <p>size of each attention head</p> required <code>`mlp_dim`</code> <p>Projection Dim in transformer after each MultiHeadAttention layer</p> required <code>`depth`</code> <p>number of patch transformer units</p> required <code>`cls_depth`</code> <p>number of transformer units applied to class attention transformer</p> required <code>`heads`</code> <p>number of attention heads</p> required <code>`channels`</code> <p>number of features/channels in the input default <code>3</code></p> required <code>`dropout_rate`</code> <p>dropout applied to MultiHeadAttention in class and patch transformers</p> required Source code in <code>k3im/cait_1d.py</code> <pre><code>def CAiT_1DModel(\n    seq_len,\n    patch_size,\n    num_classes,\n    dim,\n    dim_head,\n    mlp_dim,\n    depth,\n    cls_depth,\n    heads,\n    channels=3,\n    dropout_rate=0.0,\n):\n    \"\"\" \n    An extention of Class Attention in Image Transformers (CAiT) reimplemented for 1D Data. \n    The model expects 1D data of shape `(batch, seq_len, channels)`\n\n    Args:\n        `seq_len`: number of steps\n        `patch_size`: number steps in a patch \n        `num_classes`: output classes for classification\n        `dim`: projection dim for patches,\n        `dim_head`: size of each attention head\n        `mlp_dim`: Projection Dim in transformer after each MultiHeadAttention layer\n        `depth`: number of patch transformer units\n        `cls_depth`: number of transformer units applied to class attention transformer\n        `heads`: number of attention heads\n        `channels`: number of features/channels in the input default `3`\n        `dropout_rate`: dropout applied to MultiHeadAttention in class and patch transformers\n    \"\"\"\n    assert seq_len % patch_size == 0\n    patch_dim = channels * patch_size\n    i_p = layers.Input((seq_len, channels))\n    patches = layers.Reshape((-1, patch_dim))(i_p)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    pos_embedding = posemb_sincos_1d(patches)\n    patches += pos_embedding\n    dim = ops.shape(patches)[-1]\n    patches = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout_rate=dropout_rate)(patches)\n    _, cls_token = CLS_Token(dim)(patches)\n    cls_token = Transformer(dim, cls_depth, heads, dim_head, mlp_dim, dropout_rate=dropout_rate)(\n        cls_token, patches\n    )\n    cls_token = ops.squeeze(cls_token, axis=1)\n    o_p = layers.Dense(num_classes)(cls_token)\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> <p><code>k3im.cct_1d.CCT_1DModel</code> </p> <p>Create a Convolutional Transformer for sequences.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <p>A tuple of (seq_len, num_channels).</p> required <code>num_heads</code> <p>An integer.</p> required <code>projection_dim</code> <p>An integer representing the projection dimension.</p> required <code>kernel_size</code> <p>An integer representing the size of the convolution window.</p> required <code>stride</code> <p>An integer representing the stride of the convolution.</p> required <code>padding</code> <p>One of 'valid', 'same' or 'causal'. Causal is for decoding.</p> required <code>transformer_units</code> <p>A list of integers representing the number of units in each transformer layer.</p> required <code>stochastic_depth_rate</code> <p>A float representing the drop probability for the stochastic depth layer.</p> required <code>transformer_layers</code> <p>An integer representing the number of transformer layers.</p> required <code>num_classes</code> <p>An integer representing the number of classes for classification.</p> required <code>positional_emb</code> <p>Boolean, whether to use positional embeddings.</p> <code>False</code> Source code in <code>k3im/cct_1d.py</code> <pre><code>def CCT_1DModel(\n    input_shape,\n    num_heads,\n    projection_dim,\n    kernel_size,\n    stride,\n    padding,\n    transformer_units,\n    stochastic_depth_rate,\n    transformer_layers,\n    num_classes,\n    positional_emb=False,\n):\n    \"\"\"\n    Create a Convolutional Transformer for sequences.\n\n    Args:\n        input_shape: A tuple of (seq_len, num_channels).\n        num_heads: An integer.\n        projection_dim: An integer representing the projection dimension.\n        kernel_size: An integer representing the size of the convolution window.\n        stride: An integer representing the stride of the convolution.\n        padding: One of 'valid', 'same' or 'causal'. Causal is for decoding.\n        transformer_units: A list of integers representing the number of units\n            in each transformer layer.\n        stochastic_depth_rate: A float representing the drop probability for the\n            stochastic depth layer.\n        transformer_layers: An integer representing the number of transformer layers.\n        num_classes: An integer representing the number of classes for classification.\n        positional_emb: Boolean, whether to use positional embeddings.\n\n    \"\"\"\n    inputs = layers.Input(input_shape)\n\n    # Encode patches.\n\n    cct_tokenizer = CCTTokenizer1D(\n        kernel_size,\n        stride,\n        padding,\n        n_output_channels=[64, projection_dim],\n        n_conv_layers=2,\n    )\n    encoded_patches = cct_tokenizer(inputs)\n\n    # Apply positional embedding.\n    if positional_emb:\n        sequence_length = encoded_patches.shape[1]\n        encoded_patches += PositionEmbedding(sequence_length=sequence_length)(\n            encoded_patches\n        )\n\n    # Calculate Stochastic Depth probabilities.\n    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n\n    # Create multiple layers of the Transformer block.\n    for i in range(transformer_layers):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n\n        # Create a multi-head attention layer.\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n        )(x1, x1)\n\n        # Skip connection 1.\n        attention_output = StochasticDepth(dpr[i])(attention_output)\n        x2 = layers.Add()([attention_output, encoded_patches])\n\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n\n        # MLP.\n        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n\n        # Skip connection 2.\n        x3 = StochasticDepth(dpr[i])(x3)\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Apply sequence pooling.\n    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n    weighted_representation = SequencePooling()(representation)\n\n    # Classify outputs.\n    logits = layers.Dense(num_classes)(weighted_representation)\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=logits)\n    return model\n</code></pre> <p><code>k3im.convmixer_1d.ConvMixer1DModel</code> </p> <p>ConvMixer model for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>`seq_len`</code> <p>number of steps</p> required <code>`n_features`</code> <p>number of features/channels in the input default <code>3</code></p> required <code>`filters`</code> <p>number of filters in the convolutional stem</p> required <code>`depth`</code> <p>number of conv mixer blocks</p> required <code>`kernel_size`</code> <p>kernel size for the depthwise convolution</p> required <code>`patch_size`</code> <p>number steps in a patch</p> required <code>`num_classes`</code> <p>output classes for classification</p> required Source code in <code>k3im/convmixer_1d.py</code> <pre><code>def ConvMixer1DModel(\n    seq_len=32,\n    n_features=3,\n    filters=256,\n    depth=8,\n    kernel_size=5,\n    patch_size=2,\n    num_classes=10,\n):\n    \"\"\" \n    ConvMixer model for 1D data.\n\n    Args:\n        `seq_len`: number of steps\n        `n_features`: number of features/channels in the input default `3`\n        `filters`: number of filters in the convolutional stem\n        `depth`: number of conv mixer blocks\n        `kernel_size`: kernel size for the depthwise convolution\n        `patch_size`: number steps in a patch\n        `num_classes`: output classes for classification\n\n    \"\"\"\n    inputs = keras.Input((seq_len, n_features))\n\n    # Extract patch embeddings.\n    x = conv_stem(inputs, filters, patch_size)\n\n    # ConvMixer blocks.\n    for _ in range(depth):\n        x = conv_mixer_block(x, filters, kernel_size)\n\n    # Classification block.\n    x = layers.GlobalAvgPool1D()(x)\n    outputs = layers.Dense(num_classes)(x)\n\n    return keras.Model(inputs, outputs)\n</code></pre> <p><code>k3im.eanet_1d.EANet1DModel</code> </p> <p>Create an External Attention Network for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>`seq_len`</code> <p>number of steps</p> required <code>`patch_size`</code> <p>number steps in a patch</p> required <code>`num_classes`</code> <p>output classes for classification</p> required <code>`dim`</code> <p>projection dim for patches,</p> required <code>`depth`</code> <p>number of patch transformer units</p> required <code>`heads`</code> <p>number of attention heads</p> required <code>`mlp_dim`</code> <p>Projection Dim in transformer after each MultiHeadAttention layer</p> required <code>`dim_coefficient`</code> <p>coefficient for increasing the number of heads</p> required <code>`attention_dropout`</code> <p>dropout applied to MultiHeadAttention in class and patch transformers</p> required <code>`channels`</code> <p>number of features/channels in the input default <code>3</code></p> required Source code in <code>k3im/eanet_1d.py</code> <pre><code>def EANet1DModel(\n    seq_len,\n    patch_size,\n    num_classes,\n    dim,\n    depth,\n    heads,\n    mlp_dim,\n    dim_coefficient=4,\n    attention_dropout=0.0,\n    channels=3,\n):\n    \"\"\"\n    Create an External Attention Network for 1D data.\n\n    Args:\n        `seq_len`: number of steps\n        `patch_size`: number steps in a patch\n        `num_classes`: output classes for classification\n        `dim`: projection dim for patches,\n        `depth`: number of patch transformer units\n        `heads`: number of attention heads\n        `mlp_dim`: Projection Dim in transformer after each MultiHeadAttention layer\n        `dim_coefficient`: coefficient for increasing the number of heads\n        `attention_dropout`: dropout applied to MultiHeadAttention in class and patch transformers\n        `channels`: number of features/channels in the input default `3`\n\n    \"\"\"\n    assert seq_len % patch_size == 0\n    patch_dim = channels * patch_size\n    i_p = layers.Input((seq_len, channels))\n    patches = layers.Reshape((-1, patch_dim))(i_p)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    pos_embedding = posemb_sincos_1d(patches)\n    patches += pos_embedding\n    patches = Transformer(\n        dim=dim,\n        depth=depth,\n        heads=heads,\n        mlp_dim=mlp_dim,\n        dim_coefficient=dim_coefficient,\n        attention_dropout=attention_dropout,\n        projection_dropout=0,\n    )(patches)\n    patches = layers.GlobalAveragePooling1D(name=\"avg_pool\")(patches)\n    o_p = layers.Dense(num_classes)(patches)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> <p><code>k3im.fnet_1d.FNet1DModel</code> </p> <p>Instantiate a FNet model for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>seq_len</code> <p>An integer representing the number of steps in the input sequence.</p> required <code>patch_size</code> <p>An integer representing the number of steps in a patch (default=4).  </p> required <code>num_classes</code> <p>An integer representing the number of classes for classification.</p> required <code>dim</code> <p>An integer representing the projection dimension.</p> required <code>depth</code> <p>An integer representing the number of transformer layers.</p> required <code>channels</code> <p>An integer representing the number of channels in the input.</p> <code>3</code> <code>dropout_rate</code> <p>A float representing the dropout rate.</p> <code>0.0</code> Source code in <code>k3im/fnet_1d.py</code> <pre><code>def FNet1DModel(\n    seq_len, patch_size, num_classes, dim, depth, channels=3, dropout_rate=0.0\n):\n    \"\"\"\n    Instantiate a FNet model for 1D data.\n\n    Args:\n        seq_len: An integer representing the number of steps in the input sequence.\n        patch_size: An integer representing the number of steps in a\n            patch (default=4).  \n        num_classes: An integer representing the number of classes for classification.\n        dim: An integer representing the projection dimension.\n        depth: An integer representing the number of transformer layers.\n        channels: An integer representing the number of channels in the input.\n        dropout_rate: A float representing the dropout rate.\n    \"\"\"\n    assert seq_len % patch_size == 0\n    num_patches = seq_len // patch_size\n    patch_dim = channels * patch_size\n    i_p = layers.Input((seq_len, channels))\n    patches = layers.Reshape((-1, patch_dim))(i_p)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    pos_embedding = posemb_sincos_1d(patches)\n    patches += pos_embedding\n    dim = ops.shape(patches)[-1]\n    for _ in range(depth):\n        patches = FNetLayer(dim, dropout_rate)(patches)\n    patches = layers.GlobalAveragePooling1D(name=\"avg_pool\")(patches)\n    o_p = layers.Dense(num_classes)(patches)\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> <p><code>k3im.gmlp_1d.gMLP1DModel</code> </p> <p>Instantiate a gMLP model for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>seq_len</code> <p>An integer representing the number of steps in the input sequence.</p> required <code>patch_size</code> <p>An integer representing the number of steps in a patch (default=4).</p> required <code>num_classes</code> <p>An integer representing the number of classes for classification.</p> required <code>dim</code> <p>An integer representing the projection dimension.</p> required <code>depth</code> <p>An integer representing the number of transformer layers.</p> required <code>channels</code> <p>An integer representing the number of channels in the input.</p> <code>3</code> <code>dropout_rate</code> <p>A float representing the dropout rate.</p> <code>0.0</code> Source code in <code>k3im/gmlp_1d.py</code> <pre><code>def gMLP1DModel(\n    seq_len, patch_size, num_classes, dim, depth, channels=3, dropout_rate=0.0\n):\n    \"\"\"Instantiate a gMLP model for 1D data.\n\n    Args:\n        seq_len: An integer representing the number of steps in the input sequence.\n        patch_size: An integer representing the number of steps in a\n            patch (default=4).\n        num_classes: An integer representing the number of classes for classification.\n        dim: An integer representing the projection dimension.\n        depth: An integer representing the number of transformer layers.\n        channels: An integer representing the number of channels in the input.\n        dropout_rate: A float representing the dropout rate.\n\n        \"\"\"\n    assert seq_len % patch_size == 0\n    num_patches = seq_len // patch_size\n    patch_dim = channels * patch_size\n    i_p = layers.Input((seq_len, channels))\n    patches = layers.Reshape((-1, patch_dim))(i_p)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    pos_embedding = posemb_sincos_1d(patches)\n    patches += pos_embedding\n    dim = ops.shape(patches)[-1]\n    for _ in range(depth):\n        patches = gMLPLayer(num_patches, dim, dropout_rate)(patches)\n    patches = layers.GlobalAveragePooling1D(name=\"avg_pool\")(patches)\n    o_p = layers.Dense(num_classes)(patches)\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> <p><code>k3im.mlp_mixer_1d.Mixer1DModel</code> </p> <p>Instantiate a Mixer model for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>seq_len</code> <p>An integer representing the number of steps in the input sequence.</p> required <code>patch_size</code> <p>An integer representing the number of steps in a patch (default=4).</p> required <code>num_classes</code> <p>An integer representing the number of classes for classification.</p> required <code>dim</code> <p>An integer representing the projection dimension.</p> required <code>depth</code> <p>An integer representing the number of transformer layers.</p> required <code>channels</code> <p>An integer representing the number of channels in the input.</p> <code>3</code> <code>hidden_units</code> <p>An integer representing the number of hidden units in the MLP.</p> <code>64</code> <code>dropout_rate</code> <p>A float representing the dropout rate.</p> <code>0.0</code> Source code in <code>k3im/mlp_mixer_1d.py</code> <pre><code>def Mixer1DModel(\n    seq_len,\n    patch_size,\n    num_classes,\n    dim,\n    depth,\n    channels=3,\n    hidden_units=64,\n    dropout_rate=0.0,\n):\n    \"\"\"Instantiate a Mixer model for 1D data.\n\n    Args:\n        seq_len: An integer representing the number of steps in the input sequence.\n        patch_size: An integer representing the number of steps in a\n            patch (default=4).\n        num_classes: An integer representing the number of classes for classification.\n        dim: An integer representing the projection dimension.\n        depth: An integer representing the number of transformer layers.\n        channels: An integer representing the number of channels in the input.\n        hidden_units: An integer representing the number of hidden units in the MLP.\n        dropout_rate: A float representing the dropout rate.\n    \"\"\"\n    assert seq_len % patch_size == 0\n    num_patches = seq_len // patch_size\n    patch_dim = channels * patch_size\n    i_p = layers.Input((seq_len, channels))\n    patches = layers.Reshape((-1, patch_dim))(i_p)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    pos_embedding = posemb_sincos_1d(patches)\n    patches += pos_embedding\n    for _ in range(depth):\n        patches = MLPMixerLayer(num_patches, hidden_units, dropout_rate)(patches)\n    patches = layers.GlobalAveragePooling1D(name=\"avg_pool\")(patches)\n    o_p = layers.Dense(num_classes)(patches)\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> <p><code>k3im.simple_vit_1d.SimpleViT1DModel</code> </p> <p>Create a Simple Vision Transformer for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>`seq_len`</code> <p>number of steps</p> required <code>`patch_size`</code> <p>number steps in a patch</p> required <code>`num_classes`</code> <p>output classes for classification</p> required <code>`dim`</code> <p>projection dim for patches,</p> required <code>`depth`</code> <p>number of patch transformer units</p> required <code>`heads`</code> <p>number of attention heads</p> required <code>`mlp_dim`</code> <p>Projection Dim in transformer after each MultiHeadAttention layer</p> required <code>`channels`</code> <p>number of features/channels in the input default <code>3</code></p> required <code>`dim_head`</code> <p>size of each attention head</p> required Source code in <code>k3im/simple_vit_1d.py</code> <pre><code>def SimpleViT1DModel(\n    seq_len,\n    patch_size,\n    num_classes,\n    dim,\n    depth,\n    heads,\n    mlp_dim,\n    channels=3,\n    dim_head=64,\n):\n    \"\"\" Create a Simple Vision Transformer for 1D data.\n\n    Args:\n        `seq_len`: number of steps\n        `patch_size`: number steps in a patch\n        `num_classes`: output classes for classification\n        `dim`: projection dim for patches,\n        `depth`: number of patch transformer units\n        `heads`: number of attention heads\n        `mlp_dim`: Projection Dim in transformer after each MultiHeadAttention layer\n        `channels`: number of features/channels in the input default `3`\n        `dim_head`: size of each attention head\n    \"\"\"\n    assert seq_len % patch_size == 0\n    patch_dim = channels * patch_size\n    i_p = layers.Input((seq_len, channels))\n    patches = layers.Reshape((-1, patch_dim))(i_p)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    pos_embedding = posemb_sincos_1d(patches)\n    patches += pos_embedding\n    patches = Transformer(dim, depth, heads, dim_head, mlp_dim)(patches)\n    patches = layers.GlobalAveragePooling1D(name=\"avg_pool\")(patches)\n    o_p = layers.Dense(num_classes)(patches)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> <p><code>k3im.vit_1d.ViT1DModel</code> </p> <p>Create a Vision Transformer for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>`seq_len`</code> <p>number of steps</p> required <code>`patch_size`</code> <p>number steps in a patch</p> required <code>`num_classes`</code> <p>output classes for classification</p> required <code>`dim`</code> <p>projection dim for patches,</p> required <code>`depth`</code> <p>number of patch transformer units</p> required <code>`heads`</code> <p>number of attention heads</p> required <code>`mlp_dim`</code> <p>Projection Dim in transformer after each MultiHeadAttention layer</p> required <code>`channels`</code> <p>number of features/channels in the input default <code>3</code></p> required <code>`dim_head`</code> <p>size of each attention head</p> required Source code in <code>k3im/vit_1d.py</code> <pre><code>def ViT1DModel(\n    seq_len,\n    patch_size,\n    num_classes,\n    dim,\n    depth,\n    heads,\n    mlp_dim,\n    channels=3,\n    dim_head=64,\n):\n    \"\"\" Create a Vision Transformer for 1D data.\n\n    Args:\n        `seq_len`: number of steps\n        `patch_size`: number steps in a patch\n        `num_classes`: output classes for classification\n        `dim`: projection dim for patches,\n        `depth`: number of patch transformer units\n        `heads`: number of attention heads\n        `mlp_dim`: Projection Dim in transformer after each MultiHeadAttention layer\n        `channels`: number of features/channels in the input default `3`\n        `dim_head`: size of each attention head\n    \"\"\"\n    assert seq_len % patch_size == 0\n    patch_dim = channels * patch_size\n    i_p = layers.Input((seq_len, channels))\n    patches = layers.Reshape((-1, patch_dim))(i_p)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    num_patches = ops.shape(patches)[1]\n    patches = ClassTokenPositionEmb(num_patches, dim)(patches)\n    patches = Transformer(dim, depth, heads, dim_head, mlp_dim)(patches)\n    cls_tokens = patches[:, -1]\n    o_p = layers.Dense(num_classes)(cls_tokens)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre>"},{"location":"2d_models/","title":"2D Models","text":"<p><code>k3im.cait.CAiTModel</code> </p> <p>Create a Class-Attention in Image Transformer (CaiT) model.</p> <p>Parameters:</p> Name Type Description Default <code>`image_size`</code> <p>tuple of (height, width) of the image</p> required <code>`patch_size`</code> <p>tuple of (height, width) of the patch</p> required <code>`num_classes`</code> <p>output classes for classification</p> required <code>`dim`</code> <p>dimension of the model</p> required <code>`depth`</code> <p>depth of the model</p> required <code>`heads`</code> <p>number of heads in the model</p> required <code>`mlp_dim`</code> <p>dimension of the mlp</p> required <code>`cls_depth`</code> <p>depth of the cls token</p> required <code>`channels`</code> <p>number of channels in the image</p> required <code>`dim_head`</code> <p>dimension of the head</p> required <code>`aug`</code> <p>augmentation layer</p> required Source code in <code>k3im/cait.py</code> <pre><code>def CaiTModel(\n    image_size,\n    patch_size,\n    num_classes,\n    dim,\n    depth,\n    heads,\n    mlp_dim,\n    cls_depth,\n    channels=3,\n    dim_head=64,\n    aug=None,\n):\n    \"\"\" Create a Class-Attention in Image Transformer (CaiT) model.\n\n    Args:\n        `image_size`: tuple of (height, width) of the image\n        `patch_size`: tuple of (height, width) of the patch\n        `num_classes`: output classes for classification\n        `dim`: dimension of the model\n        `depth`: depth of the model\n        `heads`: number of heads in the model\n        `mlp_dim`: dimension of the mlp\n        `cls_depth`: depth of the cls token\n        `channels`: number of channels in the image\n        `dim_head`: dimension of the head\n        `aug`: augmentation layer\n    \"\"\"\n    image_height, image_width = pair(image_size)\n    patch_height, patch_width = pair(patch_size)\n\n    assert (\n        image_height % patch_height == 0 and image_width % patch_width == 0\n    ), \"Image dimensions must be divisible by the patch size.\"\n    patch_dim = channels * patch_height * patch_width\n\n    i_p = layers.Input((image_height, image_width, channels))\n    if aug is not None:\n        img = aug(i_p)\n    else:\n        img = i_p\n    patches = ops.image.extract_patches(img, (patch_height, patch_width))\n    patches = layers.Reshape((-1, patch_dim))(patches)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    num_patches = ops.shape(patches)[1]\n    patches = PositionEmb(num_patches, dim)(patches)\n    patches = Transformer(dim, depth, heads, dim_head, mlp_dim)(patches)\n    _, cls_token = CLS_Token(dim)(patches)\n    cls_token = Transformer(dim, cls_depth, heads, dim_head, mlp_dim)(\n        cls_token, context=patches\n    )\n    if num_classes is None:\n        model = keras.Model(inputs=i_p, outputs=cls_token)\n        return model\n\n    cls_token = ops.squeeze(cls_token, axis=1)\n    o_p = layers.Dense(num_classes)(cls_token)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> <p><code>k3im.cct.CCT</code> </p> <p>Instantiates the Compact Convolutional Transformer architecture.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <p>tuple of (height, width, channels)</p> required <code>num_heads</code> <p>number of attention heads</p> required <code>projection_dim</code> <p>projection dimension</p> required <code>kernel_size</code> <p>kernel size for the first convolutional layer</p> required <code>stride</code> <p>stride for the first convolutional layer</p> required <code>padding</code> <p>padding for the first convolutional layer</p> required <code>transformer_units</code> <p>list of units for the transformer blocks</p> required <code>stochastic_depth_rate</code> <p>dropout rate for the stochastic depth</p> required <code>transformer_layers</code> <p>number of transformer blocks</p> required <code>num_classes</code> <p>number of output classes</p> required <code>positional_emb</code> <p>boolean, whether to use positional embeddings</p> <code>False</code> <code>aug</code> <p>data augmentation</p> <code>None</code> Source code in <code>k3im/cct.py</code> <pre><code>def CCT(\n    input_shape,\n    num_heads,\n    projection_dim,\n    kernel_size,\n    stride,\n    padding,\n    transformer_units,\n    stochastic_depth_rate,\n    transformer_layers,\n    num_classes,\n    positional_emb=False,\n    aug=None\n):\n    \"\"\" Instantiates the Compact Convolutional Transformer architecture.\n\n    Args:\n        input_shape: tuple of (height, width, channels)\n        num_heads: number of attention heads\n        projection_dim: projection dimension\n        kernel_size: kernel size for the first convolutional layer\n        stride: stride for the first convolutional layer\n        padding: padding for the first convolutional layer\n        transformer_units: list of units for the transformer blocks\n        stochastic_depth_rate: dropout rate for the stochastic depth\n        transformer_layers: number of transformer blocks\n        num_classes: number of output classes\n        positional_emb: boolean, whether to use positional embeddings\n        aug: data augmentation\n\n    \"\"\"\n    inputs = layers.Input(input_shape)\n    if aug is not None:\n        img = aug(inputs)\n    else:\n        img = inputs\n    # Encode patches.\n\n    cct_tokenizer = CCTTokenizer(\n        kernel_size,\n        stride,\n        padding,\n        n_output_channels=[64, projection_dim],\n        n_conv_layers=2,\n    )\n    encoded_patches = cct_tokenizer(img)\n\n    # Apply positional embedding.\n    if positional_emb:\n        sequence_length = encoded_patches.shape[1]\n        encoded_patches += PositionEmbedding(sequence_length=sequence_length)(\n            encoded_patches\n        )\n\n    # Calculate Stochastic Depth probabilities.\n    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n\n    # Create multiple layers of the Transformer block.\n    for i in range(transformer_layers):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n\n        # Create a multi-head attention layer.\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n        )(x1, x1)\n\n        # Skip connection 1.\n        attention_output = StochasticDepth(dpr[i])(attention_output)\n        x2 = layers.Add()([attention_output, encoded_patches])\n\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n\n        # MLP.\n        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n\n        # Skip connection 2.\n        x3 = StochasticDepth(dpr[i])(x3)\n        encoded_patches = layers.Add()([x3, x2])\n    if num_classes is None:\n        model = keras.Model(inputs=inputs, outputs=encoded_patches)\n        return model\n\n    # Apply sequence pooling.\n    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n    weighted_representation = SequencePooling()(representation)\n\n    # Classify outputs.\n    logits = layers.Dense(num_classes)(weighted_representation)\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=logits)\n    return model\n</code></pre> <p><code>k3im.convmixer.ConvMixer</code> </p> <p>Instantiates the ConvMixer architecture.</p> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <p>Input image size.</p> <code>32</code> <code>filters</code> <p>Number of filters.</p> <code>256</code> <code>depth</code> <p>Depth of the network.</p> <code>8</code> <code>kernel_size</code> <p>Kernel size.</p> <code>5</code> <code>patch_size</code> <p>Patch size.</p> <code>2</code> <code>num_classes</code> <p>Number of classes.</p> <code>10</code> <code>num_channels</code> <p>Number of input channels.</p> <code>3</code> <code>aug</code> <p>Augmentation layer.</p> <code>None</code> Source code in <code>k3im/convmixer.py</code> <pre><code>def ConvMixer(\n    image_size=32,\n    filters=256,\n    depth=8,\n    kernel_size=5,\n    patch_size=2,\n    num_classes=10,\n    num_channels=3,\n    aug=None\n):\n    \"\"\"Instantiates the ConvMixer architecture.\n\n    Args:\n        image_size: Input image size.\n        filters: Number of filters.\n        depth: Depth of the network.\n        kernel_size: Kernel size.\n        patch_size: Patch size.\n        num_classes: Number of classes.\n        num_channels: Number of input channels.\n        aug: Augmentation layer.\n    \"\"\"\n    inputs = keras.Input((image_size, image_size, num_channels))\n    if aug is not None:\n        img = aug(inputs)\n    else:\n        img = inputs\n    x = layers.Rescaling(scale=1.0 / 255)(img)\n\n    # Extract patch embeddings.\n    x = conv_stem(x, filters, patch_size)\n\n    # ConvMixer blocks.\n    for _ in range(depth):\n        x = conv_mixer_block(x, filters, kernel_size)\n\n    if num_classes is None:\n        model = keras.Model(inputs=inputs, outputs=x)\n        return model\n\n\n    # Classification block.\n    x = layers.GlobalAvgPool2D()(x)\n    outputs = layers.Dense(num_classes)(x)\n\n    return keras.Model(inputs, outputs)\n</code></pre> <p><code>k3im.eanet.EANet</code> </p> <p>Instantiates the EANet architecture.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <p>tuple of (height, width, channels)</p> required <code>patch_size</code> <p>size of the patch</p> required <code>embedding_dim</code> <p>dimension of the embedding</p> required <code>num_transformer_blocks</code> <p>number of transformer blocks</p> required <code>mlp_dim</code> <p>dimension of the mlp</p> required <code>num_heads</code> <p>number of heads</p> required <code>dim_coefficient</code> <p>dimension coefficient</p> required <code>attention_dropout</code> <p>dropout rate for attention</p> required <code>projection_dropout</code> <p>dropout rate for projection</p> required <code>num_classes</code> <p>number of classes</p> required <code>aug</code> <p>augmentation layer</p> <code>None</code> Source code in <code>k3im/eanet.py</code> <pre><code>def EANet(\n    input_shape,\n    patch_size,\n    embedding_dim,\n    num_transformer_blocks,\n    mlp_dim,\n    num_heads,\n    dim_coefficient,\n    attention_dropout,\n    projection_dropout,\n    num_classes,\n    aug=None,\n):\n    \"\"\" Instantiates the EANet architecture.\n\n    Args:\n        input_shape: tuple of (height, width, channels)\n        patch_size: size of the patch\n        embedding_dim: dimension of the embedding\n        num_transformer_blocks: number of transformer blocks\n        mlp_dim: dimension of the mlp\n        num_heads: number of heads\n        dim_coefficient: dimension coefficient\n        attention_dropout: dropout rate for attention\n        projection_dropout: dropout rate for projection\n        num_classes: number of classes\n        aug: augmentation layer\n\n    \"\"\"\n    inputs = layers.Input(shape=input_shape)\n    if aug is not None:\n        img = aug(inputs)\n    else:\n        img = inputs\n    num_patches = (input_shape[0] // patch_size) ** 2  # Number of patch\n\n    # Extract patches.\n    x = PatchExtract(patch_size)(img)\n    # Create patch embedding.\n    x = PatchEmbedding(num_patches, embedding_dim)(x)\n    # Create Transformer block.\n    for _ in range(num_transformer_blocks):\n        x = transformer_encoder(\n            x,\n            embedding_dim,\n            mlp_dim,\n            num_heads,\n            dim_coefficient,\n            attention_dropout,\n            projection_dropout,\n        )\n    # if num_classes is None return model without classification head\n    if num_classes is None:\n        return keras.Model(inputs=inputs, outputs=x)\n    x = layers.GlobalAveragePooling1D()(x)\n    outputs = layers.Dense(num_classes)(x)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    return model\n</code></pre> <p><code>k3im.fnet.FNet</code> </p> <p>Instantiates the FNet architecture.</p> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <p>Image size.</p> required <code>patch_size</code> <p>Patch size.</p> required <code>embedding_dim</code> <p>Size of the embedding dimension.</p> required <code>num_blocks</code> <p>Number of blocks.</p> required <code>dropout_rate</code> <p>Dropout rate.</p> required <code>num_classes</code> <p>Number of classes to classify images into.</p> required <code>positional_encoding</code> <p>Whether to include positional encoding.</p> <code>False</code> <code>num_channels</code> <p>Number of image channels.</p> <code>3</code> <code>aug</code> <p>Image augmentation.</p> <code>None</code> Source code in <code>k3im/fnet.py</code> <pre><code>def FNetModel(\n    image_size,\n    patch_size,\n    embedding_dim,\n    num_blocks,\n    dropout_rate,\n    num_classes,\n    positional_encoding=False,\n    num_channels=3,\n    aug=None,\n):\n    \"\"\"Instantiates the FNet architecture.\n\n    Args:\n        image_size: Image size.\n        patch_size: Patch size.\n        embedding_dim: Size of the embedding dimension.\n        num_blocks: Number of blocks.\n        dropout_rate: Dropout rate.\n        num_classes: Number of classes to classify images into.\n        positional_encoding: Whether to include positional encoding.\n        num_channels: Number of image channels.\n        aug: Image augmentation.\n\n    \"\"\"\n    image_size = pair(image_size)\n    patch_size = pair(patch_size)\n    input_shape = (image_size[0], image_size[1], num_channels)\n    inputs = layers.Input(shape=input_shape)\n    img = aug(inputs) if aug else inputs\n    num_patches = (image_size[0] // patch_size[0]) * (\n        image_size[1] // patch_size[1]\n    )  # Size of the data array.\n\n    # Augment data.\n    # Create patches.\n    patches = Patches(patch_size)(img)\n    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n    x = layers.Dense(units=embedding_dim)(patches)\n    if positional_encoding:\n        x = x + PositionEmbedding(sequence_length=num_patches)(x)\n    # Process x using the module blocks.\n    for _ in range(num_blocks):\n        x = FNetLayer(embedding_dim, dropout_rate)(x)\n    # if num_classes is None return model without classification head\n    if num_classes is None:\n        return keras.Model(inputs=inputs, outputs=x)\n    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.\n    representation = layers.GlobalAveragePooling1D()(x)\n    # Apply dropout.\n    representation = layers.Dropout(rate=dropout_rate)(representation)\n    # Compute logits outputs.\n    logits = layers.Dense(num_classes)(representation)\n    # Create the Keras model.\n    return keras.Model(inputs=inputs, outputs=logits)\n</code></pre> <p><code>k3im.focalnet.FNet</code> </p> <p>Instantiates the FocalNet architecture.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <p>Image size.</p> <code>224</code> <code>patch_size</code> <p>Patch size.</p> <code>4</code> <code>num_classes</code> <p>Number of classes.</p> <code>1000</code> <code>embed_dim</code> <p>Embedding dimension.</p> <code>128</code> <code>depths</code> <p>Depths of each stage.</p> <code>[2, 2, 6, 2]</code> <code>mlp_ratio</code> <p>Ratio of mlp hidden dim to embedding dim.</p> <code>4.0</code> <code>drop_rate</code> <p>Dropout rate.</p> <code>0.0</code> <code>drop_path_rate</code> <p>Stochastic depth rate.</p> <code>0.1</code> <code>norm_layer</code> <p>Normalization layer.</p> <code>LayerNormalization</code> <code>patch_norm</code> <p>Whether to use patch norm.</p> <code>True</code> <code>focal_levels</code> <p>Number of focal levels.</p> <code>[2, 2, 3, 2]</code> <code>focal_windows</code> <p>Focal window sizes.</p> <code>[3, 2, 3, 2]</code> <code>use_conv_embed</code> <p>Whether to use conv embed.</p> <code>False</code> <code>use_layerscale</code> <p>Whether to use layer scale.</p> <code>False</code> <code>layerscale_value</code> <p>Value for layer scale.</p> <code>0.0001</code> <code>use_postln</code> <p>Whether to use post layer norm.</p> <code>False</code> <code>use_postln_in_modulation</code> <p>Whether to use post layer norm in modulation.</p> <code>False</code> <code>normalize_modulator</code> <p>Whether to normalize modulator.</p> <code>False</code> Source code in <code>k3im/focalnet.py</code> <pre><code>def FocalNet(\n    img_size=224,\n    patch_size=4,\n    num_classes=1000,\n    embed_dim=128,\n    depths=[2, 2, 6, 2],\n    mlp_ratio=4.0,\n    drop_rate=0.0,\n    drop_path_rate=0.1,\n    norm_layer=keras.layers.LayerNormalization,\n    patch_norm=True,\n    focal_levels=[2, 2, 3, 2],\n    focal_windows=[3, 2, 3, 2],\n    use_conv_embed=False,\n    use_layerscale=False,\n    layerscale_value=1e-4,\n    use_postln=False,\n    use_postln_in_modulation=False,\n    normalize_modulator=False,\n):\n    \"\"\"Instantiates the FocalNet architecture.\n\n    Args:\n        img_size: Image size.\n        patch_size: Patch size.\n        num_classes: Number of classes.\n        embed_dim: Embedding dimension.\n        depths: Depths of each stage.\n        mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n        drop_rate: Dropout rate.\n        drop_path_rate: Stochastic depth rate.\n        norm_layer: Normalization layer.\n        patch_norm: Whether to use patch norm.\n        focal_levels: Number of focal levels.\n        focal_windows: Focal window sizes.\n        use_conv_embed: Whether to use conv embed.\n        use_layerscale: Whether to use layer scale.\n        layerscale_value: Value for layer scale.\n        use_postln: Whether to use post layer norm.\n        use_postln_in_modulation: Whether to use post layer norm in modulation.\n        normalize_modulator: Whether to normalize modulator.\n    \"\"\"\n    num_layers = len(depths)\n    embed_dim = [embed_dim * (2**i) for i in range(num_layers)]\n    dpr = [\n        ops.convert_to_numpy(x) for x in ops.linspace(0.0, drop_path_rate, sum(depths))\n    ]  # stochastic depth decay rule\n\n    def _apply(x):\n        nonlocal num_classes\n        x, *patches_resolution = PatchEmbed(\n            img_size=(img_size, img_size),\n            patch_size=patch_size,\n            # in_chans=in_chans,\n            embed_dim=embed_dim[0],\n            use_conv_embed=use_conv_embed,\n            norm_layer=norm_layer if patch_norm else None,\n            is_stem=True,\n        )(x, img_size, img_size)\n        H, W = patches_resolution[0], patches_resolution[1]\n        x = keras.layers.Dropout(drop_rate)(x)\n        for i_layer in range(num_layers):\n            x, H, W = BasicLayer(\n                dim=embed_dim[i_layer],\n                out_dim=embed_dim[i_layer + 1] if (i_layer &lt; num_layers - 1) else None,\n                input_resolution=(\n                    patches_resolution[0] // (2**i_layer),\n                    patches_resolution[1] // (2**i_layer),\n                ),\n                depth=depths[i_layer],\n                mlp_ratio=mlp_ratio,\n                drop=drop_rate,\n                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n                norm_layer=norm_layer,\n                downsample=PatchEmbed if (i_layer &lt; num_layers - 1) else None,\n                focal_level=focal_levels[i_layer],\n                focal_window=focal_windows[i_layer],\n                use_conv_embed=use_conv_embed,\n                use_layerscale=use_layerscale,\n                layerscale_value=layerscale_value,\n                use_postln=use_postln,\n                use_postln_in_modulation=use_postln_in_modulation,\n                normalize_modulator=normalize_modulator,\n            )(x, H, W)\n        # if num_classes is None return model without classification head   \n        if num_classes is None:\n            return x\n        x = norm_layer(name=\"norm\")(x)  # B L C\n        x = keras.layers.GlobalAveragePooling1D()(x)  #\n        x = keras.layers.Flatten()(x)\n        num_classes = num_classes if num_classes &gt; 0 else None\n        x = keras.layers.Dense(num_classes, name=\"head\")(x)\n        return x\n\n    return _apply\n</code></pre> <p><code>k3im.focalnet.FocalNetModel</code> `::: k3im.focalnet.FocalNetModel</p> <p><code>k3im.focalnet.focalnet_tiny_srf</code> </p> <p>FocalNet-Tiny-SRF model.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <p>Image size.</p> <code>224</code> <code>**kwargs</code> <p>Other keyword arguments.</p> <code>{}</code> Source code in <code>k3im/focalnet.py</code> <pre><code>def focalnet_tiny_srf(img_size=224, **kwargs):\n    \"\"\"FocalNet-Tiny-SRF model.\n\n    Args:\n        img_size: Image size.\n        **kwargs: Other keyword arguments.\n    \"\"\"\n    model = FocalNetModel(img_size, depths=[2, 2, 6, 2], embed_dim=96, **kwargs)\n    return model\n</code></pre> <p><code>k3im.focalnet.focalnet_small_srf</code> </p> <p>FocalNet-Small-SRF model.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <p>Image size.</p> <code>224</code> <code>**kwargs</code> <p>Other keyword arguments.</p> <code>{}</code> Source code in <code>k3im/focalnet.py</code> <pre><code>def focalnet_small_srf(img_size=224, **kwargs):\n    \"\"\"FocalNet-Small-SRF model.\n\n    Args:\n        img_size: Image size.\n        **kwargs: Other keyword arguments.\n    \"\"\"\n    model = FocalNetModel(img_size, depths=[2, 2, 18, 2], embed_dim=96, **kwargs)\n    return model\n</code></pre> <p><code>k3im.focalnet.focalnet_base_srf</code> </p> <p>FocalNet-Base-SRF model.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <p>Image size.</p> <code>224</code> <code>**kwargs</code> <p>Other keyword arguments.</p> <code>{}</code> Source code in <code>k3im/focalnet.py</code> <pre><code>def focalnet_base_srf(img_size=224, **kwargs):\n    \"\"\"FocalNet-Base-SRF model.\n\n    Args:\n        img_size: Image size.\n        **kwargs: Other keyword arguments.\n    \"\"\"\n    model = FocalNetModel(img_size, depths=[2, 2, 18, 2], embed_dim=128, **kwargs)\n    return model\n</code></pre> <p><code>k3im.focalnet.focalnet_tiny_lrf</code> </p> <p>FocalNet-Tiny-LRF model.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <p>Image size.</p> <code>224</code> <code>**kwargs</code> <p>Other keyword arguments.</p> <code>{}</code> Source code in <code>k3im/focalnet.py</code> <pre><code>def focalnet_tiny_lrf(img_size=224, **kwargs):\n    \"\"\"FocalNet-Tiny-LRF model.\n\n    Args:\n        img_size: Image size.\n        **kwargs: Other keyword arguments.\n    \"\"\"\n    model = FocalNetModel(\n        img_size, depths=[2, 2, 6, 2], embed_dim=96, focal_levels=[3, 3, 3, 3], **kwargs\n    )\n    return model\n</code></pre> <p><code>k3im.focalnet.focalnet_small_lrf</code> </p> <p>FocalNet-Small-LRF model.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <p>Image size.</p> <code>224</code> <code>**kwargs</code> <p>Other keyword arguments.</p> <code>{}</code> Source code in <code>k3im/focalnet.py</code> <pre><code>def focalnet_small_lrf(img_size=224, **kwargs):\n\n    \"\"\"FocalNet-Small-LRF model.\n\n    Args:\n        img_size: Image size.\n        **kwargs: Other keyword arguments.\n    \"\"\"\n    model = FocalNetModel(\n        img_size,\n        depths=[2, 2, 18, 2],\n        embed_dim=96,\n        focal_levels=[3, 3, 3, 3],\n        **kwargs,\n    )\n\n    return model\n</code></pre> <p><code>k3im.focalnet.focalnet_base_lrf</code> </p> <p>FocalNet-Base-LRF model.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <p>Image size.</p> <code>224</code> <code>**kwargs</code> <p>Other keyword arguments.</p> <code>{}</code> Source code in <code>k3im/focalnet.py</code> <pre><code>def focalnet_base_lrf(img_size=224, **kwargs):\n    \"\"\"FocalNet-Base-LRF model.\n\n    Args:\n        img_size: Image size.\n        **kwargs: Other keyword arguments.\n    \"\"\"\n\n    model = FocalNetModel(\n        img_size,\n        depths=[2, 2, 18, 2],\n        embed_dim=128,\n        focal_levels=[3, 3, 3, 3],\n        **kwargs,\n    )\n    return model\n</code></pre> <p><code>k3im.focalnet.focalnet_tiny_iso_16</code> </p> <p>FocalNet-Tiny-ISO-16 model.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <p>Image size.</p> <code>224</code> <code>**kwargs</code> <p>Other keyword arguments.</p> <code>{}</code> Source code in <code>k3im/focalnet.py</code> <pre><code>def focalnet_tiny_iso_16(img_size=224, **kwargs):\n    \"\"\"FocalNet-Tiny-ISO-16 model.\n\n    Args:\n        img_size: Image size.\n        **kwargs: Other keyword arguments.\n\n    \"\"\"\n    model = FocalNetModel(\n        img_size,\n        depths=[12],\n        patch_size=16,\n        embed_dim=192,\n        focal_levels=[3],\n        focal_windows=[3],\n        **kwargs,\n    )\n    return model\n</code></pre> <p><code>k3im.focalnet.focalnet_small_iso_16</code> </p> <p>FocalNet-Small-ISO-16 model.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <p>Image size.</p> <code>224</code> <code>**kwargs</code> <p>Other keyword arguments.</p> <code>{}</code> Source code in <code>k3im/focalnet.py</code> <pre><code>def focalnet_small_iso_16(img_size=224, **kwargs):\n    \"\"\"FocalNet-Small-ISO-16 model.\n\n    Args:\n        img_size: Image size.\n        **kwargs: Other keyword arguments.\n\n    \"\"\"\n    model = FocalNetModel(\n        img_size,\n        depths=[12],\n        patch_size=16,\n        embed_dim=384,\n        focal_levels=[3],\n        focal_windows=[3],\n        **kwargs,\n    )\n    return model\n</code></pre> <p><code>k3im.focalnet.focalnet_base_iso_16</code> </p> <p>FocalNet-Base-ISO-16 model.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <p>Image size.</p> <code>224</code> <code>**kwargs</code> <p>Other keyword arguments.</p> <code>{}</code> Source code in <code>k3im/focalnet.py</code> <pre><code>def focalnet_base_iso_16(img_size=224, **kwargs):\n    \"\"\"FocalNet-Base-ISO-16 model.\n\n    Args:\n        img_size: Image size.\n        **kwargs: Other keyword arguments.\n\n    \"\"\"\n    model = FocalNetModel(\n        img_size,\n        depths=[12],\n        patch_size=16,\n        embed_dim=768,\n        focal_levels=[3],\n        focal_windows=[3],\n        use_layerscale=True,\n        use_postln=True,\n        **kwargs,\n    )\n    return model\n</code></pre> <p><code>k3im.focalnet.gMLP</code> </p> <p>Instantiates the gMLP architecture.</p> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <p>Image size.</p> required <code>patch_size</code> <p>Patch size.</p> required <code>embedding_dim</code> <p>Size of the embedding dimension.</p> required <code>num_blocks</code> <p>Number of blocks.</p> required <code>dropout_rate</code> <p>Dropout rate.</p> required <code>num_classes</code> <p>Number of classes to classify images into.</p> required <code>positional_encoding</code> <p>Whether to include positional encoding.</p> <code>False</code> <code>num_channels</code> <p>Number of image channels.</p> <code>3</code> <code>aug</code> <p>Image augmentation.</p> <code>None</code> Source code in <code>k3im/gmlp.py</code> <pre><code>def gMLPModel(\n    image_size,\n    patch_size,\n    embedding_dim,\n    num_blocks,\n    dropout_rate,\n    num_classes,\n    positional_encoding=False,\n    num_channels=3,\n    aug=None,\n):  \n    \"\"\"Instantiates the gMLP architecture.\n\n    Args:\n        image_size: Image size.\n        patch_size: Patch size.\n        embedding_dim: Size of the embedding dimension.\n        num_blocks: Number of blocks.\n        dropout_rate: Dropout rate.\n        num_classes: Number of classes to classify images into.\n        positional_encoding: Whether to include positional encoding.\n        num_channels: Number of image channels.\n        aug: Image augmentation.\n\n    \"\"\"\n    image_size = pair(image_size)\n    patch_size = pair(patch_size)\n    input_shape = (image_size[0], image_size[1], num_channels)\n    inputs = layers.Input(shape=input_shape)\n    if aug is not None:\n        img = aug(inputs)\n    else:\n        img = inputs\n    num_patches = (image_size[0] // patch_size[0]) * (\n        image_size[1] // patch_size[1]\n    )  # Size of the data array.\n\n    # Augment data.\n    # Create patches.\n    patches = Patches(patch_size)(img)\n    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n    x = layers.Dense(units=embedding_dim)(patches)\n    if positional_encoding:\n        x = x + PositionEmbedding(sequence_length=num_patches)(x)\n    # Process x using the module blocks.\n    for _ in range(num_blocks):\n        x = gMLPLayer(num_patches, embedding_dim, dropout_rate)(x)\n    # if num_classes is None return model without classification head\n    if num_classes is None:\n        return keras.Model(inputs=inputs, outputs=x)\n    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.\n    representation = layers.GlobalAveragePooling1D()(x)\n    # Apply dropout.\n    representation = layers.Dropout(rate=dropout_rate)(representation)\n    # Compute logits outputs.\n    logits = layers.Dense(num_classes)(representation)\n    # Create the Keras model.\n    return keras.Model(inputs=inputs, outputs=logits)\n</code></pre> <p><code>k3im.mlp_mixer.MlpMixer</code> </p> <p>MLP-Mixer</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <p>number of classes for classification head</p> <code>1000</code> <code>img_size</code> <p>input image size</p> <code>224</code> <code>in_chans</code> <p>number of input channels</p> <code>3</code> <code>patch_size</code> <p>patch size</p> <code>16</code> <code>num_blocks</code> <p>number of blocks</p> <code>8</code> <code>embed_dim</code> <p>embedding dimension</p> <code>512</code> <code>mlp_ratio</code> <p>ratio of mlp hidden dim to embedding dim</p> <code>(0.5, 4.0)</code> <code>block_layer</code> <p>block layer type (e.g. MixerBlock, ResMLPBlock, ConvMLPBlock)</p> <code>MixerBlock</code> <code>mlp_layer</code> <p>mlp layer type (e.g. Mlp, ConvMlp)</p> <code>Mlp</code> <code>norm_layer</code> <p>normalization layer type (default: partial(layers.LayerNormalization, epsilon=1e-6))</p> <code>partial(LayerNormalization, epsilon=1e-06)</code> <code>act_layer</code> <p>activation layer type (default: keras.activations.gelu)</p> <code>gelu</code> <code>drop_rate</code> <p>dropout rate</p> <code>0.0</code> <code>proj_drop_rate</code> <p>stochastic depth rate for projection</p> <code>0.0</code> <code>drop_path_rate</code> <p>stochastic depth rate for block layers</p> <code>0.0</code> <code>stem_norm</code> <p>whether to apply normalization to stem</p> <code>False</code> <code>global_pool</code> <p>global pooling type, one of 'avg', 'max' or None</p> <code>'avg'</code> Source code in <code>k3im/mlp_mixer.py</code> <pre><code>def MlpMixer(num_classes=1000,\n            img_size=224,\n            in_chans=3,\n            patch_size=16,\n            num_blocks=8,\n            embed_dim=512,\n            mlp_ratio=(0.5, 4.0),\n            block_layer=MixerBlock,\n            mlp_layer=Mlp,\n            norm_layer=partial(layers.LayerNormalization, epsilon=1e-6),\n            act_layer=ops.gelu,\n            drop_rate=0.,\n            proj_drop_rate=0.,\n            drop_path_rate=0.,\n            stem_norm=False,\n            global_pool='avg',):\n    \"\"\" MLP-Mixer\n\n        Args:\n            num_classes: number of classes for classification head\n            img_size: input image size\n            in_chans: number of input channels\n            patch_size: patch size\n            num_blocks: number of blocks\n            embed_dim: embedding dimension\n            mlp_ratio: ratio of mlp hidden dim to embedding dim\n            block_layer: block layer type (e.g. MixerBlock, ResMLPBlock, ConvMLPBlock)\n            mlp_layer: mlp layer type (e.g. Mlp, ConvMlp)\n            norm_layer: normalization layer type (default: partial(layers.LayerNormalization, epsilon=1e-6))\n            act_layer: activation layer type (default: keras.activations.gelu)\n            drop_rate: dropout rate\n            proj_drop_rate: stochastic depth rate for projection\n            drop_path_rate: stochastic depth rate for block layers\n            stem_norm: whether to apply normalization to stem\n            global_pool: global pooling type, one of 'avg', 'max' or None\n    \"\"\"\n    img_size = pair(img_size)\n    input_shape = (img_size[0], img_size[1], in_chans)\n    inputs = layers.Input(input_shape)\n    x = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer if stem_norm else None,\n            name='stem'\n        )(inputs) # stem\n    num_patches = ops.shape(x)[1]\n    for i in range(num_blocks):\n        x = block_layer(\n                embed_dim,\n                num_patches,\n                mlp_ratio,\n                mlp_layer=mlp_layer,\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                drop=proj_drop_rate,\n                drop_path=drop_path_rate,\n                name=f\"blocks.{i}\"\n            )(x)\n    x = norm_layer(name='norm')(x) # norm\n\n    if global_pool == 'avg':\n            x = ops.mean(x, axis=1)\n    x = layers.Dropout(drop_rate)(x)\n    if num_classes &gt; 0:\n        head = layers.Dense(num_classes, name='head') \n    else:\n        head = layers.Identity() # head\n    out = head(x)\n    return keras.Model(inputs=inputs, outputs=out)\n</code></pre> <p><code>k3im.simple_vit.SimpleViT</code> </p> <p>Create a Simple Vision Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>`image_size`</code> <p>tuple of (height, width) of the image</p> required <code>`patch_size`</code> <p>tuple of (height, width) of the patch</p> required <code>`num_classes`</code> <p>output classes for classification</p> required <code>`dim`</code> <p>dimension of the model</p> required <code>`depth`</code> <p>depth of the model</p> required <code>`heads`</code> <p>number of heads in the model</p> required <code>`mlp_dim`</code> <p>dimension of the mlp</p> required <code>`channels`</code> <p>number of channels in the image</p> required <code>`dim_head`</code> <p>dimension of the head</p> required <code>`pool`</code> <p>pooling type, one of (<code>mean</code>, <code>max</code>)</p> required <code>`aug`</code> <p>augmentation layer</p> required Source code in <code>k3im/simple_vit.py</code> <pre><code>def SimpleViT(\n    image_size,\n    patch_size,\n    num_classes,\n    dim,\n    depth,\n    heads,\n    mlp_dim,\n    channels=3,\n    dim_head=64,\n    pool=\"mean\",\n    aug=None,\n):\n    \"\"\" Create a Simple Vision Transformer.\n\n    Args:\n        `image_size`: tuple of (height, width) of the image\n        `patch_size`: tuple of (height, width) of the patch\n        `num_classes`: output classes for classification\n        `dim`: dimension of the model\n        `depth`: depth of the model\n        `heads`: number of heads in the model\n        `mlp_dim`: dimension of the mlp\n        `channels`: number of channels in the image\n        `dim_head`: dimension of the head\n        `pool`: pooling type, one of (`mean`, `max`)\n        `aug`: augmentation layer\n    \"\"\"\n    image_height, image_width = pair(image_size)\n    patch_height, patch_width = pair(patch_size)\n\n    assert (\n        image_height % patch_height == 0 and image_width % patch_width == 0\n    ), \"Image dimensions must be divisible by the patch size.\"\n\n    patch_dim = channels * patch_height * patch_width\n\n    i_p = layers.Input((image_height, image_width, channels))\n    if aug is not None:\n        img = aug(i_p)\n    else:\n        img = i_p\n    patches = ops.image.extract_patches(img, (patch_height, patch_width))\n    patches = layers.Reshape((-1, patch_dim))(patches)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    pos_embedding = posemb_sincos_2d(\n        h=image_height // patch_height,\n        w=image_width // patch_width,\n        dim=dim,\n    )\n    patches += pos_embedding\n    patches = Transformer(dim, depth, heads, dim_head, mlp_dim)(patches)\n    # if num_classes is None return model without classification head\n    if num_classes is None:\n        return keras.Model(inputs=i_p, outputs=patches)\n\n    if pool == \"mean\":\n        patches = layers.GlobalAveragePooling1D(name=\"avg_pool\")(patches)\n    elif pool == \"max\":\n        patches = layers.GlobalMaxPooling1D(name=\"max_pool\")(patches)\n\n    o_p = layers.Dense(num_classes)(patches)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> <p><code>k3im.vit.ViT</code> </p> <p>Create a Vision Transformer for 2D data.</p> <p>Parameters:</p> Name Type Description Default <code>`image_size`</code> <p>tuple of ints (height, width) specifying the image dimensions</p> required <code>`patch_size`</code> <p>tuple of ints (height, width) specifying the patch dimensions</p> required <code>`num_classes`</code> <p>number of classes</p> required <code>`dim`</code> <p>dimension of the transformer</p> required <code>`depth`</code> <p>number of transformer layers</p> required <code>`heads`</code> <p>number of attention heads</p> required <code>`mlp_dim`</code> <p>dimension of the mlp</p> required <code>`channels`</code> <p>number of channels in the input image</p> required <code>`dim_head`</code> <p>dimension of the head</p> required <code>`pool`</code> <p>type of pooling at the end of the network</p> required <code>`aug`</code> <p>augmentation layer</p> required Source code in <code>k3im/vit.py</code> <pre><code>def ViT(\n    image_size,\n    patch_size,\n    num_classes,\n    dim,\n    depth,\n    heads,\n    mlp_dim,\n    channels=3,\n    dim_head=64,\n    pool=\"mean\",\n    aug=None,\n):\n    \"\"\" Create a Vision Transformer for 2D data.\n\n    Args:\n        `image_size`: tuple of ints (height, width) specifying the image dimensions\n        `patch_size`: tuple of ints (height, width) specifying the patch dimensions\n        `num_classes`: number of classes\n        `dim`: dimension of the transformer\n        `depth`: number of transformer layers\n        `heads`: number of attention heads\n        `mlp_dim`: dimension of the mlp\n        `channels`: number of channels in the input image\n        `dim_head`: dimension of the head\n        `pool`: type of pooling at the end of the network\n        `aug`: augmentation layer\n    \"\"\"\n    image_height, image_width = pair(image_size)\n    patch_height, patch_width = pair(patch_size)\n\n    assert (\n        image_height % patch_height == 0 and image_width % patch_width == 0\n    ), \"Image dimensions must be divisible by the patch size.\"\n    assert pool in {\n        \"cls\",\n        \"mean\",\n    }, \"pool type must be either cls (cls token) or mean (mean pooling)\"\n    patch_dim = channels * patch_height * patch_width\n\n    i_p = layers.Input((image_height, image_width, channels))\n    if aug is not None:\n        img = aug(i_p)\n    else:\n        img = i_p\n    patches = ops.image.extract_patches(img, (patch_height, patch_width))\n    patches = layers.Reshape((-1, patch_dim))(patches)\n    patches = layers.LayerNormalization()(patches)\n    patches = layers.Dense(dim)(patches)\n    patches = layers.LayerNormalization()(patches)\n    num_patches = ops.shape(patches)[1]\n    patches = ClassTokenPositionEmb(num_patches, dim)(patches)\n    patches = Transformer(dim, depth, heads, dim_head, mlp_dim)(patches)\n    # if num_classes is None return model without classification head\n    if num_classes is None:\n        return keras.Model(inputs=i_p, outputs=patches)\n\n    if pool == \"cls\":\n        patches = patches[:, -1]\n    elif pool == \"mean\":\n        patches = layers.GlobalAveragePooling1D(name=\"max_pool\")(patches)\n\n    o_p = layers.Dense(num_classes)(patches)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre>"},{"location":"3d_models/","title":"3D Models","text":"Source code in <code>k3im/cait_3d.py</code> <pre><code>def CAiT3DModel(\n    image_size,\n    image_patch_size,\n    frames,\n    frame_patch_size,\n    num_classes,\n    dim,\n    depth,\n    cls_depth,\n    heads,\n    mlp_dim,\n    channels=3,\n    dim_head=64,\n):\n    image_height, image_width = pair(image_size)\n    patch_height, patch_width = pair(image_patch_size)\n\n    assert (\n        image_height % patch_height == 0 and image_width % patch_width == 0\n    ), \"Image dimensions must be divisible by the patch size.\"\n    assert (\n        frames % frame_patch_size == 0\n    ), \"Frames must be divisible by the frame patch size\"\n\n    nf, nh, nw = (\n        frames // frame_patch_size,\n        image_height // patch_height,\n        image_width // patch_width,\n    )\n    patch_dim = channels * patch_height * patch_width * frame_patch_size\n\n    i_p = layers.Input((frames, image_height, image_width, channels))\n    tubelets = layers.Reshape(\n        (frame_patch_size, nf, patch_height, nh, patch_width, nw, channels)\n    )(i_p)\n    tubelets = ops.transpose(tubelets, (0, 2, 4, 6, 1, 3, 5, 7))\n    tubelets = layers.Reshape((nf, nh, nw, -1))(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Dense(dim)(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Reshape((-1, dim))(tubelets)\n    tubelets = Transformer(dim, depth, heads, dim_head, mlp_dim)(tubelets)\n\n    _, cls_token = CLS_Token(dim)(tubelets)\n    cls_token = Transformer(dim, cls_depth, heads, dim_head, mlp_dim)(\n        cls_token, context=tubelets\n    )\n    cls_token = ops.squeeze(cls_token, axis=1)\n    o_p = layers.Dense(num_classes)(cls_token)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> Source code in <code>k3im/cct_3d.py</code> <pre><code>def CCT3DModel(\n    input_shape,\n    num_heads,\n    projection_dim,\n    kernel_size,\n    stride,\n    padding,\n    transformer_units,\n    stochastic_depth_rate,\n    transformer_layers,\n    num_classes,\n    positional_emb=False,\n):\n    inputs = layers.Input(input_shape)\n\n    # Encode patches.\n\n    cct_tokenizer = CCTTokenizer3D(\n        kernel_size,\n        stride,\n        padding,\n        n_output_channels=[64, projection_dim],\n        n_conv_layers=2,\n    )\n    encoded_patches = cct_tokenizer(inputs)\n\n    # Apply positional embedding.\n    if positional_emb:\n        sequence_length = encoded_patches.shape[1]\n        encoded_patches += PositionEmbedding(sequence_length=sequence_length)(\n            encoded_patches\n        )\n\n    # Calculate Stochastic Depth probabilities.\n    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n\n    # Create multiple layers of the Transformer block.\n    for i in range(transformer_layers):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n\n        # Create a multi-head attention layer.\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n        )(x1, x1)\n\n        # Skip connection 1.\n        attention_output = StochasticDepth(dpr[i])(attention_output)\n        x2 = layers.Add()([attention_output, encoded_patches])\n\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n\n        # MLP.\n        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n\n        # Skip connection 2.\n        x3 = StochasticDepth(dpr[i])(x3)\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Apply sequence pooling.\n    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n    weighted_representation = SequencePooling()(representation)\n\n    # Classify outputs.\n    logits = layers.Dense(num_classes)(weighted_representation)\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=logits)\n    return model\n</code></pre> <p>ConvMixer-256/8: https://openreview.net/pdf?id=TVHS5Y4dNvM. The hyperparameter values are taken from the paper.</p> Source code in <code>k3im/convmixer_3d.py</code> <pre><code>def ConvMixer3DModel(\n    image_size=28,\n    num_frames=28,\n    filters=256,\n    depth=8,\n    kernel_size=5,\n    kernel_depth=5,\n    patch_size=2,\n    patch_depth=2,\n    num_classes=10,\n    num_channels=3,\n):\n    \"\"\"ConvMixer-256/8: https://openreview.net/pdf?id=TVHS5Y4dNvM.\n    The hyperparameter values are taken from the paper.\n    \"\"\"\n\n    inputs = keras.Input((num_frames, image_size, image_size, num_channels))\n    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n\n    kernel_size = (kernel_depth,) + pair(kernel_size)\n    patch_size = (patch_depth,) + pair(patch_size)\n    # Extract patch embeddings.\n    x = conv_stem(x, filters, patch_size)\n\n    # ConvMixer blocks.\n    for _ in range(depth):\n        x = conv_mixer_block(x, filters, kernel_size)\n\n    # Classification block.\n    x = layers.GlobalAvgPool3D()(x)\n    outputs = layers.Dense(num_classes)(x)\n\n    return keras.Model(inputs, outputs)\n</code></pre> Source code in <code>k3im/eanet3d.py</code> <pre><code>def EANet3DModel(\n    image_size,\n    image_patch_size,\n    frames,\n    frame_patch_size,\n    num_classes,\n    dim,\n    depth,\n    heads,\n    mlp_dim,\n    channels=3,\n    dim_coefficient=4,\n    projection_dropout=0.0,\n    attention_dropout=0,\n):\n    image_height, image_width = pair(image_size)\n    patch_height, patch_width = pair(image_patch_size)\n\n    assert (\n        image_height % patch_height == 0 and image_width % patch_width == 0\n    ), \"Image dimensions must be divisible by the patch size.\"\n    assert (\n        frames % frame_patch_size == 0\n    ), \"Frames must be divisible by the frame patch size\"\n\n    nf, nh, nw = (\n        frames // frame_patch_size,\n        image_height // patch_height,\n        image_width // patch_width,\n    )\n    patch_dim = channels * patch_height * patch_width * frame_patch_size\n\n    i_p = layers.Input((frames, image_height, image_width, channels))\n    tubelets = layers.Reshape(\n        (frame_patch_size, nf, patch_height, nh, patch_width, nw, channels)\n    )(i_p)\n    tubelets = ops.transpose(tubelets, (0, 2, 4, 6, 1, 3, 5, 7))\n    tubelets = layers.Reshape((nf, nh, nw, -1))(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Dense(dim)(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Reshape((-1, dim))(tubelets)\n    tubelets = Transformer(\n        dim,\n        depth,\n        heads,\n        mlp_dim,\n        dim_coefficient=dim_coefficient,\n        projection_dropout=projection_dropout,\n        attention_dropout=attention_dropout,\n    )(tubelets)\n\n    tubelets = layers.GlobalAveragePooling1D(name=\"avg_pool\")(tubelets)\n    o_p = layers.Dense(num_classes)(tubelets)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> Source code in <code>k3im/fnet_3d.py</code> <pre><code>def FNet3DModel(\n    image_size,\n    image_patch_size,\n    frames,\n    frame_patch_size,\n    num_classes,\n    dim,\n    depth,\n    hidden_units,\n    dropout_rate,\n    channels=3,\n):\n    image_height, image_width = pair(image_size)\n    patch_height, patch_width = pair(image_patch_size)\n\n    assert (\n        image_height % patch_height == 0 and image_width % patch_width == 0\n    ), \"Image dimensions must be divisible by the patch size.\"\n    assert (\n        frames % frame_patch_size == 0\n    ), \"Frames must be divisible by the frame patch size\"\n\n    nf, nh, nw = (\n        frames // frame_patch_size,\n        image_height // patch_height,\n        image_width // patch_width,\n    )\n    patch_dim = channels * patch_height * patch_width * frame_patch_size\n\n    i_p = layers.Input((frames, image_height, image_width, channels))\n    tubelets = layers.Reshape(\n        (frame_patch_size, nf, patch_height, nh, patch_width, nw, channels)\n    )(i_p)\n    tubelets = ops.transpose(tubelets, (0, 2, 4, 6, 1, 3, 5, 7))\n    tubelets = layers.Reshape((nf, nh, nw, -1))(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Dense(dim)(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Reshape((-1, dim))(tubelets)\n    num_patches = ops.shape(tubelets)[1]\n    for _ in range(depth):\n        tubelets = FNetLayer(hidden_units, dropout_rate)(tubelets)\n    tubelets = layers.GlobalAveragePooling1D(name=\"avg_pool\")(tubelets)\n    o_p = layers.Dense(num_classes)(tubelets)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> Source code in <code>k3im/gmlp_3d.py</code> <pre><code>def gMLP3DModel(\n    image_size,\n    image_patch_size,\n    frames,\n    frame_patch_size,\n    num_classes,\n    dim,\n    depth,\n    hidden_units,\n    dropout_rate,\n    channels=3,\n):\n    image_height, image_width = pair(image_size)\n    patch_height, patch_width = pair(image_patch_size)\n\n    assert (\n        image_height % patch_height == 0 and image_width % patch_width == 0\n    ), \"Image dimensions must be divisible by the patch size.\"\n    assert (\n        frames % frame_patch_size == 0\n    ), \"Frames must be divisible by the frame patch size\"\n\n    nf, nh, nw = (\n        frames // frame_patch_size,\n        image_height // patch_height,\n        image_width // patch_width,\n    )\n    patch_dim = channels * patch_height * patch_width * frame_patch_size\n\n    i_p = layers.Input((frames, image_height, image_width, channels))\n    tubelets = layers.Reshape(\n        (frame_patch_size, nf, patch_height, nh, patch_width, nw, channels)\n    )(i_p)\n    tubelets = ops.transpose(tubelets, (0, 2, 4, 6, 1, 3, 5, 7))\n    tubelets = layers.Reshape((nf, nh, nw, -1))(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Dense(dim)(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Reshape((-1, dim))(tubelets)\n    num_patches = ops.shape(tubelets)[1]\n    for _ in range(depth):\n        tubelets = gMLPLayer(num_patches, hidden_units, dropout_rate)(tubelets)\n    tubelets = layers.GlobalAveragePooling1D(name=\"avg_pool\")(tubelets)\n    o_p = layers.Dense(num_classes)(tubelets)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> Source code in <code>k3im/mlp_mixer_3d.py</code> <pre><code>def MLPMixer3DModel(\n    image_size,\n    image_patch_size,\n    frames,\n    frame_patch_size,\n    num_classes,\n    dim,\n    depth,\n    hidden_units,\n    dropout_rate,\n    channels=3,\n):\n    image_height, image_width = pair(image_size)\n    patch_height, patch_width = pair(image_patch_size)\n\n    assert (\n        image_height % patch_height == 0 and image_width % patch_width == 0\n    ), \"Image dimensions must be divisible by the patch size.\"\n    assert (\n        frames % frame_patch_size == 0\n    ), \"Frames must be divisible by the frame patch size\"\n\n    nf, nh, nw = (\n        frames // frame_patch_size,\n        image_height // patch_height,\n        image_width // patch_width,\n    )\n    patch_dim = channels * patch_height * patch_width * frame_patch_size\n\n    i_p = layers.Input((frames, image_height, image_width, channels))\n    tubelets = layers.Reshape(\n        (frame_patch_size, nf, patch_height, nh, patch_width, nw, channels)\n    )(i_p)\n    tubelets = ops.transpose(tubelets, (0, 2, 4, 6, 1, 3, 5, 7))\n    tubelets = layers.Reshape((nf, nh, nw, -1))(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Dense(dim)(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Reshape((-1, dim))(tubelets)\n    num_patches = ops.shape(tubelets)[1]\n    for _ in range(depth):\n        tubelets = MLPMixerLayer(num_patches, hidden_units, dropout_rate)(tubelets)\n    tubelets = layers.GlobalAveragePooling1D(name=\"avg_pool\")(tubelets)\n    o_p = layers.Dense(num_classes)(tubelets)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> Source code in <code>k3im/simple_vit_3d.py</code> <pre><code>def SimpleViT3DModel(\n    image_size,\n    image_patch_size,\n    frames,\n    frame_patch_size,\n    num_classes,\n    dim,\n    depth,\n    heads,\n    mlp_dim,\n    channels=3,\n    dim_head=64,\n):\n    image_height, image_width = pair(image_size)\n    patch_height, patch_width = pair(image_patch_size)\n\n    assert (\n        image_height % patch_height == 0 and image_width % patch_width == 0\n    ), \"Image dimensions must be divisible by the patch size.\"\n    assert (\n        frames % frame_patch_size == 0\n    ), \"Frames must be divisible by the frame patch size\"\n\n    nf, nh, nw = (\n        frames // frame_patch_size,\n        image_height // patch_height,\n        image_width // patch_width,\n    )\n    patch_dim = channels * patch_height * patch_width * frame_patch_size\n\n    i_p = layers.Input((frames, image_height, image_width, channels))\n    tubelets = layers.Reshape(\n        (frame_patch_size, nf, patch_height, nh, patch_width, nw, channels)\n    )(i_p)\n    tubelets = ops.transpose(tubelets, (0, 2, 4, 6, 1, 3, 5, 7))\n    tubelets = layers.Reshape((nf, nh, nw, -1))(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Dense(dim)(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    # pos_embedding = posemb_sincos_3d(tubelets)\n    tubelets = layers.Reshape((-1, dim))(tubelets)\n    # tubelets += pos_embedding\n    tubelets = Transformer(dim, depth, heads, dim_head, mlp_dim)(tubelets)\n\n    tubelets = layers.GlobalAveragePooling1D(name=\"avg_pool\")(tubelets)\n    o_p = layers.Dense(num_classes)(tubelets)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> Source code in <code>k3im/vit_3d.py</code> <pre><code>def ViT3DModel(\n    image_size,\n    image_patch_size,\n    frames,\n    frame_patch_size,\n    num_classes,\n    dim,\n    depth,\n    heads,\n    mlp_dim,\n    pool,\n    channels=3,\n    dim_head=64,\n):\n    image_height, image_width = pair(image_size)\n    patch_height, patch_width = pair(image_patch_size)\n\n    assert (\n        image_height % patch_height == 0 and image_width % patch_width == 0\n    ), \"Image dimensions must be divisible by the patch size.\"\n    assert (\n        frames % frame_patch_size == 0\n    ), \"Frames must be divisible by the frame patch size\"\n\n    assert pool in {\n        \"cls\",\n        \"mean\",\n    }, \"pool type must be either cls (cls token) or mean (mean pooling)\"\n\n    nf, nh, nw = (\n        frames // frame_patch_size,\n        image_height // patch_height,\n        image_width // patch_width,\n    )\n    patch_dim = channels * patch_height * patch_width * frame_patch_size\n\n    i_p = layers.Input((frames, image_height, image_width, channels))\n    tubelets = layers.Reshape(\n        (frame_patch_size, nf, patch_height, nh, patch_width, nw, channels)\n    )(i_p)\n    tubelets = ops.transpose(tubelets, (0, 2, 4, 6, 1, 3, 5, 7))\n    tubelets = layers.Reshape((nf, nh, nw, -1))(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Dense(dim)(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Reshape((-1, dim))(tubelets)\n    num_patches = ops.shape(tubelets)[1]\n    tubelets = ClassTokenPositionEmb(num_patches, dim)(tubelets)\n    tubelets = Transformer(dim, depth, heads, dim_head, mlp_dim)(tubelets)\n    if pool == \"mean\":\n        tubelets = layers.GlobalAveragePooling1D(name=\"avg_pool\")(tubelets)\n    else:\n        tubelets = tubelets[:, -1]\n    o_p = layers.Dense(num_classes)(tubelets)\n\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre>"},{"location":"layers/","title":"Layers, Blocks, Tokenizers, etc.:","text":"<p>             Bases: <code>Layer</code></p> Source code in <code>k3im/cct_1d.py</code> <pre><code>class CCTTokenizer1D(layers.Layer):\n    def __init__(\n        self,\n        kernel_size,\n        stride,\n        padding,\n        pooling_kernel_size=3,\n        pooling_stride=2,\n        n_conv_layers=1,\n        n_output_channels=[64],\n        max_pool=True,\n        activation=\"relu\",\n        conv_bias=False,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        assert n_conv_layers == len(n_output_channels)\n\n        # This is our tokenizer.\n        self.conv_model = keras.Sequential()\n        for i in range(n_conv_layers):\n            self.conv_model.add(\n                layers.Conv1D(\n                    n_output_channels[i],\n                    kernel_size,\n                    stride,\n                    padding=\"valid\",\n                    use_bias=conv_bias,\n                    activation=activation,\n                    kernel_initializer=\"he_normal\",\n                )\n            )\n            self.conv_model.add(layers.ZeroPadding1D(padding))\n            if max_pool:\n                self.conv_model.add(\n                    layers.MaxPooling1D(pooling_kernel_size, pooling_stride, \"same\")\n                )\n\n    def call(self, images):\n        outputs = self.conv_model(images)\n\n        return outputs\n</code></pre> <p>             Bases: <code>Layer</code></p> Source code in <code>k3im/cct_3d.py</code> <pre><code>class CCTTokenizer3D(layers.Layer):\n    def __init__(\n        self,\n        kernel_size,\n        stride,\n        padding,\n        pooling_kernel_size=3,\n        pooling_stride=2,\n        n_conv_layers=1,\n        n_output_channels=[64],\n        max_pool=True,\n        activation=\"relu\",\n        conv_bias=False,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        assert n_conv_layers == len(n_output_channels)\n\n        # This is our tokenizer.\n        self.conv_model = keras.Sequential()\n        for i in range(n_conv_layers):\n            self.conv_model.add(\n                layers.Conv3D(\n                    n_output_channels[i],\n                    kernel_size,\n                    stride,\n                    padding=\"valid\",\n                    use_bias=conv_bias,\n                    activation=activation,\n                    kernel_initializer=\"he_normal\",\n                )\n            )\n            self.conv_model.add(layers.ZeroPadding3D(padding))\n            if max_pool:\n                self.conv_model.add(\n                    layers.MaxPooling3D(pooling_kernel_size, pooling_stride, \"same\")\n                )\n\n    def call(self, images):\n        outputs = self.conv_model(images)\n        # After passing the images through our mini-network the spatial dimensions\n        # are flattened to form sequences.\n        reshaped = keras.ops.reshape(\n            outputs,\n            (\n                -1,\n                keras.ops.shape(outputs)[1]\n                * keras.ops.shape(outputs)[2]\n                * keras.ops.shape(outputs)[3],\n                keras.ops.shape(outputs)[-1],\n            ),\n        )\n        return reshaped\n</code></pre> <p>             Bases: <code>Layer</code></p> Source code in <code>k3im/cct_1d.py</code> <pre><code>class SequencePooling(layers.Layer):\n    def __init__(self):\n        super().__init__()\n        self.attention = layers.Dense(1)\n\n    def call(self, x):\n        attention_weights = keras.ops.softmax(self.attention(x), axis=1)\n        attention_weights = keras.ops.transpose(attention_weights, axes=(0, 2, 1))\n        weighted_representation = keras.ops.matmul(attention_weights, x)\n        return keras.ops.squeeze(weighted_representation, -2)\n</code></pre> <p>             Bases: <code>Layer</code></p> Source code in <code>k3im/cct.py</code> <pre><code>class PositionEmbedding(keras.layers.Layer):\n    def __init__(\n        self,\n        sequence_length,\n        initializer=\"glorot_uniform\",\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        if sequence_length is None:\n            raise ValueError(\"`sequence_length` must be an Integer, received `None`.\")\n        self.sequence_length = int(sequence_length)\n        self.initializer = keras.initializers.get(initializer)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                \"sequence_length\": self.sequence_length,\n                \"initializer\": keras.initializers.serialize(self.initializer),\n            }\n        )\n        return config\n\n    def build(self, input_shape):\n        feature_size = input_shape[-1]\n        self.position_embeddings = self.add_weight(\n            name=\"embeddings\",\n            shape=[self.sequence_length, feature_size],\n            initializer=self.initializer,\n            trainable=True,\n        )\n\n        super().build(input_shape)\n\n    def call(self, inputs, start_index=0):\n        shape = keras.ops.shape(inputs)\n        feature_length = shape[-1]\n        sequence_length = shape[-2]\n        # trim to match the length of the input sequence, which might be less\n        # than the sequence_length of the layer.\n        position_embeddings = keras.ops.convert_to_tensor(self.position_embeddings)\n        position_embeddings = keras.ops.slice(\n            position_embeddings,\n            (start_index, 0),\n            (sequence_length, feature_length),\n        )\n        return keras.ops.broadcast_to(position_embeddings, shape)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n</code></pre> Source code in <code>k3im/commons.py</code> <pre><code>def FeedForward(dim, hidden_dim, dropout=0.0):\n    return keras.Sequential(\n        [\n            layers.LayerNormalization(),\n            layers.Dense(hidden_dim, activation=\"gelu\"),\n            layers.Dropout(dropout),\n            layers.Dense(dim),\n            layers.Dropout(dropout),\n        ]\n    )\n</code></pre> Source code in <code>k3im/convmixer_1d.py</code> <pre><code>def conv_mixer_block(x, filters: int, kernel_size: int):\n    # Depthwise convolution.\n    x0 = x\n    x = layers.DepthwiseConv1D(kernel_size=kernel_size, padding=\"same\")(x)\n    x = layers.Add()([activation_block(x), x0])  # Residual.\n\n    # Pointwise convolution.\n    x = layers.Conv1D(filters, kernel_size=1)(x)\n    x = activation_block(x)\n\n    return x\n</code></pre> Source code in <code>k3im/convmixer_3d.py</code> <pre><code>def conv_mixer_block(x, filters: int, kernel_size: int):\n    # Depthwise convolution.\n    x0 = x\n    x = Conv2Plus1D(filters, kernel_size=kernel_size, padding=\"same\")(x)\n    x = layers.Add()([activation_block(x), x0])  # Residual.\n\n    # Pointwise convolution.\n    x = layers.Conv3D(filters, kernel_size=1)(x)\n    x = activation_block(x)\n\n    return x\n</code></pre> Source code in <code>k3im/convmixer_3d.py</code> <pre><code>def conv_mixer_block(x, filters: int, kernel_size: int):\n    # Depthwise convolution.\n    x0 = x\n    x = Conv2Plus1D(filters, kernel_size=kernel_size, padding=\"same\")(x)\n    x = layers.Add()([activation_block(x), x0])  # Residual.\n\n    # Pointwise convolution.\n    x = layers.Conv3D(filters, kernel_size=1)(x)\n    x = activation_block(x)\n\n    return x\n</code></pre> Source code in <code>k3im/cross_vit.py</code> <pre><code>def ProjectInOut(dim_in, dim_out, fn):\n    need_projection = dim_in != dim_out\n\n    def _apply(x, *args, **kwargs):\n        if need_projection:\n            x = layers.Dense(dim_out)(x)\n        x = fn(x, *args, **kwargs)\n        if need_projection:\n            x = layers.Dense(dim_in)(x)\n        return x\n\n    return _apply\n</code></pre> Source code in <code>k3im/cross_vit.py</code> <pre><code>def CrossTransformer(sm_dim, lg_dim, depth, heads, dim_head, dropout):\n    def _apply(sm_tokens, lg_tokens):\n        (sm_cls, sm_patch_tokens), (lg_cls, lg_patch_tokens) = map(\n            lambda t: (t[:, -1:], t[:, :-1]), (sm_tokens, lg_tokens)\n        )\n        sm_cls = (\n            ProjectInOut(\n                sm_dim,\n                lg_dim,\n                Transformer(\n                    lg_dim,\n                    depth=depth,\n                    heads=heads,\n                    dim_head=dim_head,\n                    mlp_dim=0,\n                    dropout=dropout,\n                    cross=True,\n                ),\n            )(sm_cls, context=lg_patch_tokens, kv_include_self=True)\n            + sm_cls\n        )\n        lg_cls = (\n            ProjectInOut(\n                lg_dim,\n                sm_dim,\n                Transformer(\n                    sm_dim,\n                    depth=depth,\n                    heads=heads,\n                    dim_head=dim_head,\n                    mlp_dim=0,\n                    dropout=dropout,\n                    cross=True,\n                ),\n            )(lg_cls, context=sm_patch_tokens, kv_include_self=True)\n            + lg_cls\n        )\n        sm_tokens = ops.concatenate((sm_cls, sm_patch_tokens), axis=1)\n        lg_tokens = ops.concatenate((lg_cls, lg_patch_tokens), axis=1)\n        return sm_tokens, lg_tokens\n\n    return _apply\n</code></pre> Source code in <code>k3im/cross_vit.py</code> <pre><code>def MultiScaleEncoder(\n    *,\n    depth,\n    sm_dim,\n    lg_dim,\n    sm_enc_params,\n    lg_enc_params,\n    cross_attn_heads,\n    cross_attn_depth,\n    cross_attn_dim_head=64,\n    dropout=0.0\n):\n    def _apply(sm_tokens, lg_tokens):\n        for _ in range(depth):\n            sm_tokens = Transformer(dim=sm_dim, dropout=dropout, **sm_enc_params)(\n                sm_tokens\n            )\n            lg_tokens = Transformer(dim=lg_dim, dropout=dropout, **lg_enc_params)(\n                lg_tokens\n            )\n            sm_tokens, lg_tokens = CrossTransformer(\n                sm_dim=sm_dim,\n                lg_dim=lg_dim,\n                depth=cross_attn_depth,\n                heads=cross_attn_heads,\n                dim_head=cross_attn_dim_head,\n                dropout=dropout,\n            )(sm_tokens, lg_tokens)\n        return sm_tokens, lg_tokens\n\n    return _apply\n</code></pre> Source code in <code>k3im/cross_vit.py</code> <pre><code>def ImageEmbedder(*, dim, image_size, patch_size, channels, dropout=0.0):\n    image_height, image_width = pair(image_size)\n    patch_height, patch_width = pair(patch_size)\n\n    assert (\n        image_height % patch_height == 0 and image_width % patch_width == 0\n    ), \"Image dimensions must be divisible by the patch size.\"\n    patch_dim = channels * patch_height * patch_width\n\n    def _apply(x):\n        patches = ops.image.extract_patches(x, (patch_height, patch_width))\n        patches = layers.Reshape((-1, patch_dim))(patches)\n        patches = layers.LayerNormalization()(patches)\n        patches = layers.Dense(dim)(patches)\n        patches = layers.LayerNormalization()(patches)\n        patches, _ = CLS_Token(dim)(patches)\n        num_patches = ops.shape(patches)[1]\n        patches = PositionEmb(num_patches, dim)(patches)\n        patches = layers.Dropout(dropout)(patches)\n        return patches\n\n    return _apply\n</code></pre> Source code in <code>k3im/eanet_1d.py</code> <pre><code>def ExternalAttention(\n    dim,\n    num_heads,\n    dim_coefficient=4,\n    attention_dropout=0,\n    projection_dropout=0,\n):\n    assert dim % num_heads == 0\n\n    def _apply(x):\n        nonlocal num_heads\n        _, num_patch, channel = x.shape\n        num_heads = num_heads * dim_coefficient\n        x = layers.Dense(int(dim * dim_coefficient))(x)\n        # create tensor [batch_size, num_patches, num_heads, dim*dim_coefficient//num_heads]\n        x = ops.reshape(\n            x, (-1, num_patch, num_heads, dim * dim_coefficient // num_heads)\n        )\n        x = ops.transpose(x, axes=[0, 2, 1, 3])\n        # a linear layer M_k\n        attn = layers.Dense(dim // dim_coefficient)(x)\n        # normalize attention map\n        attn = layers.Softmax(axis=2)(attn)\n        # dobule-normalization\n        attn = layers.Lambda(\n            lambda attn: ops.divide(\n                attn,\n                ops.convert_to_tensor(1e-9) + ops.sum(attn, axis=-1, keepdims=True),\n            )\n        )(attn)\n        attn = layers.Dropout(attention_dropout)(attn)\n        # a linear layer M_v\n        x = layers.Dense(dim * dim_coefficient // num_heads)(attn)\n        x = ops.transpose(x, axes=[0, 2, 1, 3])\n        x = ops.reshape(x, [-1, num_patch, dim * dim_coefficient])\n        # a linear layer to project original dim\n        x = layers.Dense(dim)(x)\n        x = layers.Dropout(projection_dropout)(x)\n        return x\n\n    return _apply\n</code></pre> <p>             Bases: <code>Layer</code></p> <p>Ported from: https://keras.io/examples/vision/mlp_image_classification/</p> Source code in <code>k3im/fnet_1d.py</code> <pre><code>class FNetLayer(layers.Layer):\n    \"\"\"\n    Ported from: https://keras.io/examples/vision/mlp_image_classification/\n    \"\"\"\n    def __init__(self, embedding_dim, dropout_rate, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(units=embedding_dim, activation=\"gelu\"),\n                layers.Dropout(rate=dropout_rate),\n                layers.Dense(units=embedding_dim),\n            ]\n        )\n\n        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n\n    def call(self, inputs):\n        # Apply fourier transformations.\n        real_part = inputs\n        im_part = keras.ops.zeros_like(inputs)\n        x = keras.ops.fft2((real_part, im_part))[0]\n        # Add skip connection.\n        x = x + inputs\n        # Apply layer normalization.\n        x = self.normalize1(x)\n        # Apply Feedfowrad network.\n        x_ffn = self.ffn(x)\n        # Add skip connection.\n        x = x + x_ffn\n        # Apply layer normalization.\n        return self.normalize2(x)\n</code></pre> <p>             Bases: <code>Layer</code></p> Source code in <code>k3im/focalnet.py</code> <pre><code>class FocalModulation(keras.layers.Layer):\n    def __init__(\n        self,\n        dim,\n        focal_window,\n        focal_level,\n        focal_factor=2,\n        bias=True,\n        proj_drop=0.0,\n        use_postln_in_modulation=True,\n        normalize_modulator=False,\n        prefix=None,\n    ):\n        if prefix is not None:\n            prefix = prefix + \".modulation\"\n            name = prefix  # + str(int(K.get_uid(prefix)) - 1)\n        else:\n            name = \"focal_modulation\"\n\n        super(FocalModulation, self).__init__(name=name)\n        self.focal_level = focal_level\n        self.use_postln_in_modulation = use_postln_in_modulation\n        self.normalize_modulator = normalize_modulator\n\n        self.f = keras.layers.Dense(\n            2 * dim + (focal_level + 1), use_bias=bias, name=f\"{name}.f\"\n        )\n\n        self.h = keras.layers.Conv2D(\n            dim, kernel_size=1, strides=1, use_bias=bias, name=f\"{name}.h\"\n        )\n\n        self.act = keras.activations.gelu\n        self.proj = keras.layers.Dense(dim, name=f\"{name}.proj\")\n        self.proj_drop = keras.layers.Dropout(proj_drop)\n        self.map = {f\"{name}.f\": self.f, f\"{name}.h\": self.h, f\"{name}.proj\": self.proj}\n\n        self.focal_layers = []\n\n        self.kernel_sizes = []\n        for k in range(self.focal_level):\n            _name = f\"{prefix}.focal_layers.\"\n            _name = _name + str(K.get_uid(_name) - 1)\n            # print(name)\n            kernel_size = focal_factor * k + focal_window\n            _layer = keras.layers.Conv2D(\n                dim,\n                kernel_size=kernel_size,\n                strides=1,\n                groups=dim,\n                use_bias=False,\n                padding=\"Same\",\n                activation=self.act,\n                name=_name,\n            )\n            self.map[_name] = _layer\n            self.focal_layers.append(_layer)\n            self.kernel_sizes.append(kernel_size)\n        if self.use_postln_in_modulation:\n            self.ln = keras.layers.LayerNormalization(name=f\"{prefix}.norm\")\n            self.map[\"norm\"] = self.ln\n\n    def call(self, x):\n        \"\"\"\n        Args:\n            x: input features with shape of (B, H, W, C)\n        \"\"\"\n        C = x.shape[-1]\n        x = self.f(x)\n        q, ctx, self.gates = ops.split(x, [C, 2 * C], -1)  # from numpy docs\n        ctx_all = 0\n        for l in range(self.focal_level):\n            ctx = self.focal_layers[l](ctx)\n            ctx_all = ctx_all + ops.multiply(ctx, self.gates[:, :, :, l : l + 1])\n        ctx = ops.mean(ctx, 1, keepdims=True)\n        ctx = ops.mean(ctx, 2, keepdims=True)\n        ctx_global = self.act(ctx)\n        ctx_all = ctx_all + ctx_global * self.gates[:, :, :, self.focal_level :]\n        if self.normalize_modulator:\n            ctx_all = ctx_all / (self.focal_level + 1)\n        modulator = self.h(ctx_all)\n        x_out = q * modulator\n        if self.use_postln_in_modulation:\n            x_out = self.ln(x_out)\n        x_out = self.proj(x_out)\n        x_out = self.proj_drop(x_out)\n        return x_out\n\n    def _get_layer(self, name):\n        return self.map[name]\n</code></pre> <p>             Bases: <code>Layer</code></p> <p>https://keras.io/examples/vision/mlp_image_classification/</p> Source code in <code>k3im/gmlp_1d.py</code> <pre><code>class gMLPLayer(layers.Layer):\n    \"\"\"\n    https://keras.io/examples/vision/mlp_image_classification/\n    \"\"\"\n    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.channel_projection1 = keras.Sequential(\n            [\n                layers.Dense(units=embedding_dim * 2, activation=\"gelu\"),\n                layers.Dropout(rate=dropout_rate),\n            ]\n        )\n\n        self.channel_projection2 = layers.Dense(units=embedding_dim)\n\n        self.spatial_projection = layers.Dense(\n            units=num_patches, bias_initializer=\"Ones\"\n        )\n\n        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n\n    def spatial_gating_unit(self, x):\n        # Split x along the channel dimensions.\n        # Tensors u and v will in the shape of [batch_size, num_patchs, embedding_dim].\n        u, v = keras.ops.split(x, indices_or_sections=2, axis=2)\n        # Apply layer normalization.\n        v = self.normalize2(v)\n        # Apply spatial projection.\n        v_channels = keras.ops.transpose(v, axes=(0, 2, 1))\n        v_projected = self.spatial_projection(v_channels)\n        v_projected = keras.ops.transpose(v_projected, axes=(0, 2, 1))\n        # Apply element-wise multiplication.\n        return u * v_projected\n\n    def call(self, inputs):\n        # Apply layer normalization.\n        x = self.normalize1(inputs)\n        # Apply the first channel projection. x_projected shape: [batch_size, num_patches, embedding_dim * 2].\n        x_projected = self.channel_projection1(x)\n        # Apply the spatial gating unit. x_spatial shape: [batch_size, num_patches, embedding_dim].\n        x_spatial = self.spatial_gating_unit(x_projected)\n        # Apply the second channel projection. x_projected shape: [batch_size, num_patches, embedding_dim].\n        x_projected = self.channel_projection2(x_spatial)\n        # Add skip connection.\n        return x + x_projected\n</code></pre> <p>             Bases: <code>Layer</code></p> <p>https://keras.io/examples/vision/mlp_image_classification/</p> Source code in <code>k3im/mlp_mixer_1d.py</code> <pre><code>class MLPMixerLayer(layers.Layer):\n    \"\"\"\n    https://keras.io/examples/vision/mlp_image_classification/\n    \"\"\"\n    def __init__(self, num_patches, hidden_units, dropout_rate, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.mlp1 = keras.Sequential(\n            [\n                layers.Dense(units=num_patches, activation=\"gelu\"),\n                layers.Dense(units=num_patches),\n                layers.Dropout(rate=dropout_rate),\n            ]\n        )\n        self.mlp2 = keras.Sequential(\n            [\n                layers.Dense(units=num_patches, activation=\"gelu\"),\n                layers.Dense(units=hidden_units),\n                layers.Dropout(rate=dropout_rate),\n            ]\n        )\n        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n\n    def build(self, input_shape):\n        return super().build(input_shape)\n\n    def call(self, inputs):\n        # Apply layer normalization.\n        x = self.normalize(inputs)\n        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n        x_channels = keras.ops.transpose(x, axes=(0, 2, 1))\n        # Apply mlp1 on each channel independently.\n        mlp1_outputs = self.mlp1(x_channels)\n        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n        mlp1_outputs = keras.ops.transpose(mlp1_outputs, axes=(0, 2, 1))\n        # Add skip connection.\n        x = mlp1_outputs + inputs\n        # Apply layer normalization.\n        x_patches = self.normalize(x)\n        # Apply mlp2 on each patch independtenly.\n        mlp2_outputs = self.mlp2(x_patches)\n        # Add skip connection.\n        x = x + mlp2_outputs\n        return x\n</code></pre> <p>             Bases: <code>Layer</code></p> Source code in <code>k3im/mlp_mixer.py</code> <pre><code>class DropPath(layers.Layer):\n    def __init__(self, rate=0.5, seed=None, **kwargs):\n        super().__init__(**kwargs)\n        self.rate = rate\n        self._seed_val = seed\n        self.seed = keras.random.SeedGenerator(seed=seed)\n\n    def call(self, x, training=None):\n        if self.rate == 0.0 or not training:\n            return x\n        else:\n            batch_size = x.shape[0] or ops.shape(x)[0]\n            drop_map_shape = (batch_size,) + (1,) * (len(x.shape) - 1)\n            drop_map = ops.cast(\n                keras.random.uniform(drop_map_shape, seed=self.seed) &gt; self.rate,\n                x.dtype,\n            )\n            x = x / (1.0 - self.rate)\n            x = x * drop_map\n            return x\n</code></pre> <p>             Bases: <code>Layer</code></p> Source code in <code>k3im/simple_vit_with_register_tokens.py</code> <pre><code>class RegisterTokens(layers.Layer):\n    def __init__(self, num_register_tokens, dim):\n        super().__init__()\n        self.register_tokens = self.add_weight(\n            [1, num_register_tokens, dim],\n            initializer=\"random_normal\",\n            dtype=\"float32\",\n            trainable=True,\n        )\n\n    def call(self, x):\n        b = ops.shape(x)[0]\n        tokens = ops.repeat(self.register_tokens, b, axis=0)\n        patches = ops.concatenate([x, tokens], axis=1)\n        return patches\n</code></pre> <p>             Bases: <code>Layer</code></p> Source code in <code>k3im/swint.py</code> <pre><code>class WindowAttention(layers.Layer):\n    def __init__(\n        self,\n        dim,\n        window_size,\n        num_heads,\n        qkv_bias=True,\n        dropout_rate=0.0,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n        self.dropout = layers.Dropout(dropout_rate)\n        self.proj = layers.Dense(dim)\n\n        num_window_elements = (2 * self.window_size[0] - 1) * (\n            2 * self.window_size[1] - 1\n        )\n        self.relative_position_bias_table = self.add_weight(\n            shape=(num_window_elements, self.num_heads),\n            initializer=keras.initializers.Zeros(),\n            trainable=True,\n        )\n        coords_h = np.arange(self.window_size[0])\n        coords_w = np.arange(self.window_size[1])\n        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n        coords = np.stack(coords_matrix)\n        coords_flatten = coords.reshape(2, -1)\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        relative_coords = relative_coords.transpose([1, 2, 0])\n        relative_coords[:, :, 0] += self.window_size[0] - 1\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)\n\n        self.relative_position_index = keras.Variable(\n            initializer=relative_position_index,\n            shape=relative_position_index.shape,\n            dtype=\"int\",\n            trainable=False,\n        )\n\n    def call(self, x, mask=None):\n        _, size, channels = x.shape\n        head_dim = channels // self.num_heads\n        x_qkv = self.qkv(x)\n        x_qkv = ops.reshape(x_qkv, (-1, size, 3, self.num_heads, head_dim))\n        x_qkv = ops.transpose(x_qkv, (2, 0, 3, 1, 4))\n        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n        q = q * self.scale\n        k = ops.transpose(k, (0, 1, 3, 2))\n        attn = q @ k\n\n        num_window_elements = self.window_size[0] * self.window_size[1]\n        relative_position_index_flat = ops.reshape(self.relative_position_index, (-1,))\n        relative_position_bias = ops.take(\n            self.relative_position_bias_table,\n            relative_position_index_flat,\n            axis=0,\n        )\n        relative_position_bias = ops.reshape(\n            relative_position_bias,\n            (num_window_elements, num_window_elements, -1),\n        )\n        relative_position_bias = ops.transpose(relative_position_bias, (2, 0, 1))\n        attn = attn + ops.expand_dims(relative_position_bias, axis=0)\n\n        if mask is not None:\n            nW = mask.shape[0]\n            mask_float = ops.cast(\n                ops.expand_dims(ops.expand_dims(mask, axis=1), axis=0),\n                \"float32\",\n            )\n            attn = ops.reshape(attn, (-1, nW, self.num_heads, size, size)) + mask_float\n            attn = ops.reshape(attn, (-1, self.num_heads, size, size))\n            attn = keras.activations.softmax(attn, axis=-1)\n        else:\n            attn = keras.activations.softmax(attn, axis=-1)\n        attn = self.dropout(attn)\n\n        x_qkv = attn @ v\n        x_qkv = ops.transpose(x_qkv, (0, 2, 1, 3))\n        x_qkv = ops.reshape(x_qkv, (-1, size, channels))\n        x_qkv = self.proj(x_qkv)\n        x_qkv = self.dropout(x_qkv)\n        return x_qkv\n</code></pre> <p>             Bases: <code>Layer</code></p> Source code in <code>k3im/swint.py</code> <pre><code>class SwinTransformer(layers.Layer):\n    def __init__(\n        self,\n        dim,\n        num_patch,\n        num_heads,\n        window_size=7,\n        shift_size=0,\n        num_mlp=1024,\n        qkv_bias=True,\n        dropout_rate=0.0,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self.dim = dim  # number of input dimensions\n        self.num_patch = num_patch  # number of embedded patches\n        self.num_heads = num_heads  # number of attention heads\n        self.window_size = window_size  # size of window\n        self.shift_size = shift_size  # size of window shift\n        self.num_mlp = num_mlp  # number of MLP nodes\n\n        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n        self.attn = WindowAttention(\n            dim,\n            window_size=(self.window_size, self.window_size),\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            dropout_rate=dropout_rate,\n        )\n        self.drop_path = layers.Dropout(dropout_rate)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n\n        self.mlp = FeedForward(dim, num_mlp, dropout=dropout_rate)\n\n        if min(self.num_patch) &lt; self.window_size:\n            self.shift_size = 0\n            self.window_size = min(self.num_patch)\n\n    def build(self, input_shape):\n        if self.shift_size == 0:\n            self.attn_mask = None\n        else:\n            height, width = self.num_patch\n            h_slices = (\n                slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None),\n            )\n            w_slices = (\n                slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None),\n            )\n            mask_array = np.zeros((1, height, width, 1))\n            count = 0\n            for h in h_slices:\n                for w in w_slices:\n                    mask_array[:, h, w, :] = count\n                    count += 1\n            mask_array = ops.convert_to_tensor(mask_array)\n\n            # mask array to windows\n            mask_windows = window_partition(mask_array, self.window_size)\n            mask_windows = ops.reshape(\n                mask_windows, [-1, self.window_size * self.window_size]\n            )\n            attn_mask = ops.expand_dims(mask_windows, axis=1) - ops.expand_dims(\n                mask_windows, axis=2\n            )\n            attn_mask = ops.where(attn_mask != 0, -100.0, attn_mask)\n            attn_mask = ops.where(attn_mask == 0, 0.0, attn_mask)\n            self.attn_mask = keras.Variable(\n                initializer=attn_mask,\n                shape=attn_mask.shape,\n                dtype=attn_mask.dtype,\n                trainable=False,\n            )\n\n    def call(self, x, training=False):\n        height, width = self.num_patch\n        _, num_patches_before, channels = x.shape\n        x_skip = x\n        x = self.norm1(x)\n        x = ops.reshape(x, (-1, height, width, channels))\n        if self.shift_size &gt; 0:\n            shifted_x = ops.roll(\n                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n            )\n        else:\n            shifted_x = x\n\n        x_windows = window_partition(shifted_x, self.window_size)\n        x_windows = ops.reshape(\n            x_windows, (-1, self.window_size * self.window_size, channels)\n        )\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n\n        attn_windows = ops.reshape(\n            attn_windows,\n            (-1, self.window_size, self.window_size, channels),\n        )\n        shifted_x = window_reverse(\n            attn_windows, self.window_size, height, width, channels\n        )\n        if self.shift_size &gt; 0:\n            x = ops.roll(\n                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n            )\n        else:\n            x = shifted_x\n\n        x = ops.reshape(x, (-1, height * width, channels))\n        x = self.drop_path(x, training=training)\n        x = x_skip + x\n        x_skip = x\n        x = self.norm2(x)\n        x = self.mlp(x)\n        x = self.drop_path(x)\n        x = x_skip + x\n        return x\n</code></pre> <p>             Bases: <code>Layer</code></p> Source code in <code>k3im/swint.py</code> <pre><code>class PatchMerging(keras.layers.Layer):\n    def __init__(self, num_patch, embed_dim):\n        super().__init__()\n        self.num_patch = num_patch\n        self.embed_dim = embed_dim\n        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n\n    def call(self, x):\n        height, width = self.num_patch\n        _, _, C = x.shape\n        x = ops.reshape(x, (-1, height, width, C))\n        x0 = x[:, 0::2, 0::2, :]\n        x1 = x[:, 1::2, 0::2, :]\n        x2 = x[:, 0::2, 1::2, :]\n        x3 = x[:, 1::2, 1::2, :]\n        x = ops.concatenate((x0, x1, x2, x3), axis=-1)\n        x = ops.reshape(x, (-1, (height // 2) * (width // 2), 4 * C))\n        return self.linear_trans(x)\n</code></pre> Source code in <code>k3im/token_learner.py</code> <pre><code>def TokenLearner(inputs, number_of_tokens):\n    # Layer normalize the inputs.\n    x = layers.LayerNormalization()(inputs)  # (B, H, W, C)\n\n    # Applying Conv2D =&gt; Reshape =&gt; Permute\n    # The reshape and permute is done to help with the next steps of\n    # multiplication and Global Average Pooling.\n    attention_maps = keras.Sequential(\n        [\n            # 3 layers of conv with gelu activation as suggested\n            # in the paper.\n            layers.Conv2D(\n                filters=number_of_tokens,\n                kernel_size=(3, 3),\n                activation=ops.gelu,\n                padding=\"same\",\n                use_bias=False,\n            ),\n            layers.Conv2D(\n                filters=number_of_tokens,\n                kernel_size=(3, 3),\n                activation=ops.gelu,\n                padding=\"same\",\n                use_bias=False,\n            ),\n            layers.Conv2D(\n                filters=number_of_tokens,\n                kernel_size=(3, 3),\n                activation=ops.gelu,\n                padding=\"same\",\n                use_bias=False,\n            ),\n            # This conv layer will generate the attention maps\n            layers.Conv2D(\n                filters=number_of_tokens,\n                kernel_size=(3, 3),\n                activation=\"sigmoid\",  # Note sigmoid for [0, 1] output\n                padding=\"same\",\n                use_bias=False,\n            ),\n            # Reshape and Permute\n            layers.Reshape((-1, number_of_tokens)),  # (B, H*W, num_of_tokens)\n            layers.Permute((2, 1)),\n        ]\n    )(\n        x\n    )  # (B, num_of_tokens, H*W)\n\n    # Reshape the input to align it with the output of the conv block.\n    num_filters = inputs.shape[-1]\n    inputs = layers.Reshape((1, -1, num_filters))(inputs)  # inputs == (B, 1, H*W, C)\n\n    # Element-Wise multiplication of the attention maps and the inputs\n    attended_inputs = (\n        ops.expand_dims(attention_maps, axis=-1) * inputs\n    )  # (B, num_tokens, H*W, C)\n\n    # Global average pooling the element wise multiplication result.\n    outputs = ops.mean(attended_inputs, axis=2)  # (B, num_tokens, C)\n    return outputs\n</code></pre> <p>             Bases: <code>Layer</code></p> Source code in <code>k3im/video_eanet.py</code> <pre><code>class ClassTokenSpatial(layers.Layer):\n    def __init__(self, sequence_length, output_dim, num_frames, **kwargs):\n        super().__init__(**kwargs)\n        self.num_frames = num_frames\n        self.class_token = self.add_weight(\n            shape=[1, 1, 1, output_dim], initializer=\"random_normal\"\n        )\n        self.sequence_length = sequence_length\n        self.output_dim = output_dim\n\n    def call(self, inputs):\n        batch, length = ops.shape(inputs)[0], ops.shape(inputs)[1]\n\n        cls_token = ops.repeat(self.class_token, batch, axis=0)\n        cls_token = ops.repeat(cls_token, self.num_frames, axis=1)\n        patches = ops.concatenate([inputs, cls_token], axis=2)\n        return patches\n</code></pre> <p>             Bases: <code>Layer</code></p> Source code in <code>k3im/video_eanet.py</code> <pre><code>class ClassTokenTemporal(layers.Layer):\n    def __init__(self, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.class_token = self.add_weight(\n            shape=[1, 1, output_dim], initializer=\"random_normal\"\n        )\n        self.output_dim = output_dim\n\n    def call(self, inputs):\n        batch, length = ops.shape(inputs)[0], ops.shape(inputs)[1]\n\n        cls_token = ops.repeat(self.class_token, batch, axis=0)\n        patches = ops.concatenate([inputs, cls_token], axis=1)\n        return patches\n</code></pre> <p>             Bases: <code>Layer</code></p> Source code in <code>k3im/vit_1d.py</code> <pre><code>class ClassTokenPositionEmb(layers.Layer):\n    def __init__(self, sequence_length, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.position_embeddings = layers.Embedding(\n            input_dim=(sequence_length + 1), output_dim=output_dim\n        )\n        self.class_token = self.add_weight(\n            shape=[1, 1, output_dim], initializer=\"random_normal\"\n        )\n        self.sequence_length = sequence_length\n        self.output_dim = output_dim\n\n    def call(self, inputs):\n        batch, length = ops.shape(inputs)[0], ops.shape(inputs)[1]\n\n        cls_token = ops.repeat(self.class_token, batch, axis=0)\n        patches = ops.concatenate([inputs, cls_token], axis=1)\n        positions = ops.arange(start=0, stop=(length + 1), step=1)\n        embedded_positions = self.position_embeddings(positions)\n        return patches + embedded_positions\n</code></pre>"},{"location":"layers/#k3im.focalnet.FocalModulation.call","title":"<code>call(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>input features with shape of (B, H, W, C)</p> required Source code in <code>k3im/focalnet.py</code> <pre><code>def call(self, x):\n    \"\"\"\n    Args:\n        x: input features with shape of (B, H, W, C)\n    \"\"\"\n    C = x.shape[-1]\n    x = self.f(x)\n    q, ctx, self.gates = ops.split(x, [C, 2 * C], -1)  # from numpy docs\n    ctx_all = 0\n    for l in range(self.focal_level):\n        ctx = self.focal_layers[l](ctx)\n        ctx_all = ctx_all + ops.multiply(ctx, self.gates[:, :, :, l : l + 1])\n    ctx = ops.mean(ctx, 1, keepdims=True)\n    ctx = ops.mean(ctx, 2, keepdims=True)\n    ctx_global = self.act(ctx)\n    ctx_all = ctx_all + ctx_global * self.gates[:, :, :, self.focal_level :]\n    if self.normalize_modulator:\n        ctx_all = ctx_all / (self.focal_level + 1)\n    modulator = self.h(ctx_all)\n    x_out = q * modulator\n    if self.use_postln_in_modulation:\n        x_out = self.ln(x_out)\n    x_out = self.proj(x_out)\n    x_out = self.proj_drop(x_out)\n    return x_out\n</code></pre>"},{"location":"space_time_models/","title":"Space-Time Models","text":"Source code in <code>k3im/video_eanet.py</code> <pre><code>def VideoEANet(\n    image_size,\n    image_patch_size,\n    frames,\n    frame_patch_size,\n    num_classes,\n    dim,\n    spatial_depth,\n    temporal_depth,\n    heads,\n    mlp_dim,\n    pool=\"cls\",\n    channels=3,\n    dim_coefficient=4,\n    projection_dropout=0.0,\n    attention_dropout=0,\n    emb_dropout=0.0,\n):\n    image_height, image_width = pair(image_size)\n    patch_height, patch_width = pair(image_patch_size)\n\n    assert (\n        image_height % patch_height == 0 and image_width % patch_width == 0\n    ), \"Image dimensions must be divisible by the patch size.\"\n    assert (\n        frames % frame_patch_size == 0\n    ), \"Frames must be divisible by the frame patch size\"\n\n    nf, nh, nw = (\n        frames // frame_patch_size,\n        image_height // patch_height,\n        image_width // patch_width,\n    )\n    patch_dim = channels * patch_height * patch_width * frame_patch_size\n\n    i_p = layers.Input((frames, image_height, image_width, channels))\n    tubelets = layers.Reshape(\n        (frame_patch_size, nf, patch_height, nh, patch_width, nw, channels)\n    )(i_p)\n    tubelets = ops.transpose(tubelets, (0, 2, 4, 6, 1, 3, 5, 7))\n    tubelets = layers.Reshape((nf, nh, nw, -1))(tubelets)\n    tubelets = layers.Reshape((nf, nh * nw, -1))(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Dense(dim)(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    seq_len, num_frames = ops.shape(tubelets)[2], ops.shape(tubelets)[1]\n    tubelets = ClassTokenSpatial(\n        sequence_length=seq_len, output_dim=dim, num_frames=num_frames\n    )(tubelets)\n    tubelets = layers.Dropout(emb_dropout)(tubelets)\n    seq_len = ops.shape(tubelets)[2]\n    tubelets = ops.reshape(tubelets, (-1, seq_len, dim))  ######### ERRRRRRR\n    tubelets = Transformer(\n        dim,\n        spatial_depth,\n        heads,\n        mlp_dim,\n        dim_coefficient=dim_coefficient,\n        projection_dropout=projection_dropout,\n        attention_dropout=attention_dropout,\n    )(tubelets)\n    tubelets = ops.reshape(tubelets, (-1, num_frames, seq_len, dim))  ######### ERRRRRRR\n    if pool == \"mean\":\n        tubelets = ops.mean(tubelets, axis=2)\n    else:\n        tubelets = tubelets[:, :, -1]\n    tubelets = ClassTokenTemporal(dim)(tubelets)\n    tubelets = Transformer(\n        dim,\n        temporal_depth,\n        heads,\n        mlp_dim,\n        dim_coefficient=dim_coefficient,\n        projection_dropout=projection_dropout,\n        attention_dropout=attention_dropout,\n    )(tubelets)\n    if pool == \"mean\":\n        tubelets = ops.mean(tubelets, axis=1)\n    else:\n        tubelets = tubelets[:, -1]\n    o_p = layers.Dense(num_classes)(tubelets)\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> Source code in <code>k3im/video_mixer.py</code> <pre><code>def VideoMixerModel(\n    image_size,\n    image_patch_size,\n    frames,\n    frame_patch_size,\n    num_classes,\n    dim,\n    spatial_depth,\n    temporal_depth,\n    mlp_dim,\n    channels=3,\n    spatial_dropout=0.0,\n    emb_dropout=0.0,\n):\n    image_height, image_width = pair(image_size)\n    patch_height, patch_width = pair(image_patch_size)\n\n    assert (\n        image_height % patch_height == 0 and image_width % patch_width == 0\n    ), \"Image dimensions must be divisible by the patch size.\"\n    assert (\n        frames % frame_patch_size == 0\n    ), \"Frames must be divisible by the frame patch size\"\n\n    nf, nh, nw = (\n        frames // frame_patch_size,\n        image_height // patch_height,\n        image_width // patch_width,\n    )\n    patch_dim = channels * patch_height * patch_width * frame_patch_size\n\n    i_p = layers.Input((frames, image_height, image_width, channels))\n    tubelets = layers.Reshape(\n        (frame_patch_size, nf, patch_height, nh, patch_width, nw, channels)\n    )(i_p)\n    tubelets = ops.transpose(tubelets, (0, 2, 4, 6, 1, 3, 5, 7))\n    tubelets = layers.Reshape((nf, nh, nw, -1))(tubelets)\n    tubelets = layers.Reshape((nf, nh * nw, -1))(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Dense(dim)(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    seq_len, num_frames = ops.shape(tubelets)[2], ops.shape(tubelets)[1]\n    tubelets = layers.Dropout(emb_dropout)(tubelets)\n    tubelets = ops.reshape(tubelets, (-1, seq_len, dim))\n    for _ in range(spatial_depth):\n        tubelets = MLPMixerLayer(seq_len, mlp_dim, spatial_dropout)(tubelets)\n    tubelets = ops.reshape(tubelets, (-1, num_frames, seq_len, dim))\n    tubelets = ops.mean(tubelets, axis=2)\n    seq_len = ops.shape(tubelets)[1]\n    for _ in range(temporal_depth):\n        tubelets = MLPMixerLayer(seq_len, mlp_dim, spatial_dropout)(tubelets)\n    tubelets = ops.mean(tubelets, axis=1)\n    o_p = layers.Dense(num_classes)(tubelets)\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre> Source code in <code>k3im/vivit.py</code> <pre><code>def ViViT(\n    image_size,\n    image_patch_size,\n    frames,\n    frame_patch_size,\n    num_classes,\n    dim,\n    spatial_depth,\n    temporal_depth,\n    heads,\n    mlp_dim,\n    pool=\"cls\",\n    channels=3,\n    dim_head=64,\n    dropout=0.0,\n    emb_dropout=0.0,\n):\n    image_height, image_width = pair(image_size)\n    patch_height, patch_width = pair(image_patch_size)\n\n    assert (\n        image_height % patch_height == 0 and image_width % patch_width == 0\n    ), \"Image dimensions must be divisible by the patch size.\"\n    assert (\n        frames % frame_patch_size == 0\n    ), \"Frames must be divisible by the frame patch size\"\n\n    nf, nh, nw = (\n        frames // frame_patch_size,\n        image_height // patch_height,\n        image_width // patch_width,\n    )\n    patch_dim = channels * patch_height * patch_width * frame_patch_size\n\n    i_p = layers.Input((frames, image_height, image_width, channels))\n    tubelets = layers.Reshape(\n        (frame_patch_size, nf, patch_height, nh, patch_width, nw, channels)\n    )(i_p)\n    tubelets = ops.transpose(tubelets, (0, 2, 4, 6, 1, 3, 5, 7))\n    tubelets = layers.Reshape((nf, nh, nw, -1))(tubelets)\n    tubelets = layers.Reshape((nf, nh * nw, -1))(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    tubelets = layers.Dense(dim)(tubelets)\n    tubelets = layers.LayerNormalization()(tubelets)\n    seq_len, num_frames = ops.shape(tubelets)[2], ops.shape(tubelets)[1]\n    tubelets = ClassTokenSpatial(\n        sequence_length=seq_len, output_dim=dim, num_frames=num_frames\n    )(tubelets)\n    tubelets = layers.Dropout(emb_dropout)(tubelets)\n    seq_len = ops.shape(tubelets)[2]\n    tubelets = ops.reshape(tubelets, (-1, seq_len, dim))  ######### ERRRRRRR\n    tubelets = Transformer(\n        dim,\n        spatial_depth,\n        heads,\n        dim_head,\n        mlp_dim,\n    )(tubelets)\n    tubelets = ops.reshape(tubelets, (-1, num_frames, seq_len, dim))  ######### ERRRRRRR\n    if pool == \"mean\":\n        tubelets = ops.mean(tubelets, axis=2)\n    else:\n        tubelets = tubelets[:, :, -1]\n    tubelets = ClassTokenTemporal(dim)(tubelets)\n    tubelets = Transformer(dim, temporal_depth, heads, dim_head, mlp_dim)(tubelets)\n    if pool == \"mean\":\n        tubelets = ops.mean(tubelets, axis=1)\n    else:\n        tubelets = tubelets[:, -1]\n    o_p = layers.Dense(num_classes)(tubelets)\n    return keras.Model(inputs=i_p, outputs=o_p)\n</code></pre>"}]}